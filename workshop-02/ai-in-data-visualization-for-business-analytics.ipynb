{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Welcome and Introducation\n",
    "\n",
    "## Who we are\n",
    "![Team](https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/team.png)\n",
    "\n",
    "## Our collaborative objectives\n",
    "> Implement a series of AI-integrated workshops on data-driven decision making, aiming to enrich prospective students' learning experiences and encourage increased participation and success in the business analytics program. \n",
    " \n",
    "\n",
    "---\n",
    "\n",
    "## Workshop agenda\n",
    "0. [Setup](#Setup)\n",
    "1. [Welcome and Introduction](#1-welcome-and-introducation)\n",
    "2. [Consent and Pre-measurements](#2-consent-and-pre-measurements)\n",
    "3. [Lecture](#3-lecture)\n",
    "4. [Demo](#4-demo)\n",
    "5. [Wrap-up](#5-wrap-up)\n",
    "6. [Post-measurements](#6-post-measurements) \n",
    "\n",
    "---\n",
    "\n",
    "## Workshop series\n",
    "<div style=\"color: lightgrey;\">\n",
    "* Workshop 1:\n",
    "    * Understand fundamental AI concepts\n",
    "    * Identify specific AI use cases in business scenarios, with a focus on AI in data modeling\n",
    "    * Experiment with AI tool (i.e., LLMs-based agents) deployment as an assistant for business analysts\n",
    "        * Analytics task: Understanding your customer\n",
    "            * customer segementation\n",
    "            * co-buying behaviors\n",
    "    * Recognize AI's dual role in enhancing business analytics - both in data modeling and as an assitant for business analysts\n",
    "</div>\n",
    "\n",
    "* Workshop 2:\n",
    "    * Understand the role of data visualization in business analytics \n",
    "    * Identify basic chart types and when to use\n",
    "    * Experiment with AI tool (i.e., LLMs-based agents) deployment as an assistant for business analysts\n",
    "        * Analytics task: Understanding your product\n",
    "            * historical performance\n",
    "            * revenue prediction\n",
    "            * demand forecasting\n",
    "            * price optimization\n",
    "\n",
    "\n",
    "<div style=\"color: lightgrey;\">\n",
    "* Workshop 3:\n",
    "    * Understand general AI ethical challenges\n",
    "    * Identify AI ethics in business analytics\n",
    "    * Gain insights from a business analyst professional\n",
    "        * Explore the career paths\n",
    "        * Identify essential skills\n",
    "        * Understand AI’s impact on the industry\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Consent and Pre-measurements\n",
    "\n",
    "## Consent Form\n",
    "- [Link](https://docs.google.com/forms/d/e/1FAIpQLSc3DNPf8Yu99O4crWiQMdGzJli8R4QlmlMsL04aYDG2Y2vMvw/viewform?usp=share_link)\n",
    "- <img src=\"https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/consent.png\" alt=\"QR\" width=\"200\">\n",
    "\n",
    "## Pre-assessments (prior knowledge)\n",
    "- [Link](https://ncsu.qualtrics.com/jfe/form/SV_6Wf0Ix7EdB9NDpA)\n",
    "- <img src=\"https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/pre_assess.png\" alt=\"QR\" width=\"200\">\n",
    "\n",
    "\n",
    "## Pre-survey (prior experience)\n",
    "* [Link](https://ncsu.qualtrics.com/jfe/form/SV_3JdVSGjdZP7qjSm)\n",
    "* <img src=\"https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/pre_survey.png\" alt=\"QR\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Lecture\n",
    "\n",
    "## Today's Topics\n",
    "- What is the role of data visualization (Data → Insights → Visualization)?\n",
    "- What are the basic chart types and when to use them?\n",
    "\n",
    "\n",
    "## Data → Insights → Visualization\n",
    "![Data → Insights → Visualization](https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/data_visualization.png)\n",
    "[Image credit: Brandingmag](https://www.brandingmag.com/2020/05/26/the-power-of-data-visualization-in-finding-insights/)\n",
    "\n",
    "\n",
    "## Basic chart types & When to use\n",
    "![Basic chart types & When to use](https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/basic_charts.png)\n",
    "[Image credit: eazyBI](https://eazybi.com/blog/data-visualization-and-chart-types#:~:text=Bar%20charts%20are%20good%20for,never%20for%20comparisons%20or%20distributions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Demo\n",
    "## (1). Dataset\n",
    "\n",
    "In this section, we will explore how to deploy AI tools (i.e., LLMs-based agents) to conduct both traditional and machine-learning-based business analytics using data from the [Online Retail dataset](https://archive.ics.uci.edu/dataset/352/online+retail). \n",
    "\n",
    "> The Online Retail dataset, available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu), contains transactional data from a UK-based online retail store. It is commonly used for data analysis, machine learning, and business intelligence applications such as customer segmentation, sales forecasting, and market basket analysis.\n",
    "\n",
    "### Dataset Overview\n",
    "* Source: Transactions from an anonymous UK-based e-commerce store.\n",
    "* Time Period: One year (from December 2010 to December 2011).\n",
    "* Number of Records: 541,909 transactions.\n",
    "* Number of Features: 8 columns describing each transaction.\n",
    "\n",
    "### Features in the Dataset\n",
    "* InvoiceNo – Unique identifier for each transaction.\n",
    "* StockCode – Product code associated with each item.\n",
    "* Description – Name or description of the product.\n",
    "* Quantity – Number of items purchased in the transaction.\n",
    "* InvoiceDate – Date and time of the transaction.\n",
    "* UnitPrice – Price of a single unit of the product (in GBP).\n",
    "* CustomerID – Unique identifier for customers (missing for some transactions).\n",
    "* Country – Country of the customer making the purchase.\n",
    "\n",
    "## (2). Your Tasks as a Business Analyst\n",
    "> From your manager:<br>\n",
    "> \"I'd like you to lead an initiative to deepen our understanding of our product to inform our strategic decisions in the coming quarters.\"\n",
    "\n",
    "## (3). Use AI Tools (i.e., LLMs - \"Cluade\") as Your Assistant\n",
    "[Sample inquiry chat history](https://claude.ai/share/72a38f9e-db77-46dc-907e-f781e9289bcc)\n",
    "\n",
    "## (4). Experiment on AI-assisted Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Performance\n",
    "### a. Historical Performance Summary (non-machine-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "import os\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_theme()\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "def load_and_prepare_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and prepare the retail dataset for product analysis.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    print(\"\\nDataset Overview:\")\n",
    "    print(f\"- Shape: {df.shape}\")\n",
    "    print(f\"- Columns: {', '.join(df.columns)}\")\n",
    "    \n",
    "    # Basic data cleaning\n",
    "    # Remove canceled orders (negative quantities)\n",
    "    df_cleaned = df[df['Quantity'] > 0]\n",
    "    \n",
    "    # Remove missing values\n",
    "    df_cleaned = df_cleaned.dropna(subset=['StockCode', 'Description'])\n",
    "    \n",
    "    # Filter out non-product items\n",
    "    non_product_codes = ['POST', 'D', 'M', 'CRUK', 'C2', 'DOT', 'BANK CHARGES']\n",
    "    df_cleaned = df_cleaned[~df_cleaned['StockCode'].isin(non_product_codes)]\n",
    "    \n",
    "    # Filter out items with generic descriptions\n",
    "    generic_descriptions = ['DISCOUNT', 'POSTAGE', 'MANUAL', 'SAMPLES', 'TEST', 'ADJUST']\n",
    "    pattern = '|'.join(generic_descriptions)\n",
    "    df_cleaned = df_cleaned[~df_cleaned['Description'].str.contains(pattern, case=False, na=False)]\n",
    "    \n",
    "    # Add total value column\n",
    "    df_cleaned['Revenue'] = df_cleaned['Quantity'] * df_cleaned['UnitPrice']\n",
    "    \n",
    "    # Convert InvoiceDate to datetime if it's not already\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_cleaned['InvoiceDate']):\n",
    "        df_cleaned['InvoiceDate'] = pd.to_datetime(df_cleaned['InvoiceDate'])\n",
    "    \n",
    "    # Extract date components\n",
    "    df_cleaned['Year'] = df_cleaned['InvoiceDate'].dt.year\n",
    "    df_cleaned['Month'] = df_cleaned['InvoiceDate'].dt.month\n",
    "    df_cleaned['Day'] = df_cleaned['InvoiceDate'].dt.day\n",
    "    df_cleaned['WeekDay'] = df_cleaned['InvoiceDate'].dt.day_name()\n",
    "    \n",
    "    # Create month-year field for time series analysis\n",
    "    df_cleaned['YearMonth'] = df_cleaned['InvoiceDate'].dt.strftime('%Y-%m')\n",
    "    \n",
    "    print(f\"\\nAfter cleaning: {df_cleaned.shape[0]} transactions\")\n",
    "    return df_cleaned\n",
    "\n",
    "def analyze_top_products(df, n=20):\n",
    "    \"\"\"\n",
    "    Analyze top products by revenue, quantity, and transaction count.\n",
    "    \"\"\"\n",
    "    # Group by product and calculate metrics\n",
    "    product_metrics = df.groupby(['StockCode', 'Description']).agg({\n",
    "        'Revenue': 'sum',\n",
    "        'Quantity': 'sum',\n",
    "        'InvoiceNo': 'nunique',\n",
    "        'UnitPrice': 'mean',\n",
    "        'Country': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate average quantity per transaction\n",
    "    product_metrics['AvgQuantityPerTransaction'] = (\n",
    "        product_metrics['Quantity'] / product_metrics['InvoiceNo']\n",
    "    )\n",
    "    \n",
    "    # Calculate revenue share\n",
    "    total_revenue = product_metrics['Revenue'].sum()\n",
    "    product_metrics['RevenueShare'] = (product_metrics['Revenue'] / total_revenue * 100)\n",
    "    \n",
    "    # Sort by revenue (descending)\n",
    "    product_metrics = product_metrics.sort_values('Revenue', ascending=False)\n",
    "    \n",
    "    # Add rank columns\n",
    "    product_metrics['RevenueRank'] = range(1, len(product_metrics) + 1)\n",
    "    product_metrics['QuantityRank'] = product_metrics['Quantity'].rank(ascending=False).astype(int)\n",
    "    product_metrics['TransactionRank'] = product_metrics['InvoiceNo'].rank(ascending=False).astype(int)\n",
    "    \n",
    "    # Analyze top products\n",
    "    top_products_revenue = product_metrics.head(n).copy()\n",
    "    \n",
    "    print(f\"\\nTop {n} Products by Revenue:\")\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    print(top_products_revenue[['Description', 'Revenue', 'Quantity', 'InvoiceNo', 'UnitPrice', 'RevenueShare']].head(10))\n",
    "    \n",
    "    # Create figure directory if it doesn't exist\n",
    "    if not os.path.exists('figure'):\n",
    "        os.makedirs('figure')\n",
    "    \n",
    "    # Plot top products by revenue\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.barh(top_products_revenue['Description'].str[:30], top_products_revenue['Revenue'])\n",
    "    plt.xlabel('Revenue')\n",
    "    plt.ylabel('Product')\n",
    "    plt.title(f'Top {n} Products by Revenue')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to show highest revenue at the top\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(width * 1.01, bar.get_y() + bar.get_height()/2, \n",
    "                f'£{width:,.0f}', ha='left', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/top_products_revenue.png', dpi=300)\n",
    "    print(\"Top products by revenue saved as 'figure/top_products_revenue.png'\")\n",
    "    \n",
    "    # Visualize revenue distribution (Pareto analysis)\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Calculate cumulative revenue share\n",
    "    product_metrics_sorted = product_metrics.sort_values('Revenue', ascending=False).copy()\n",
    "    product_metrics_sorted['CumulativeRevenueShare'] = product_metrics_sorted['RevenueShare'].cumsum()\n",
    "    \n",
    "    # Plot\n",
    "    ax1 = plt.gca()\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # Bar chart of revenue share\n",
    "    ax1.bar(range(min(100, len(product_metrics_sorted))), \n",
    "           product_metrics_sorted['RevenueShare'].head(100), \n",
    "           alpha=0.7, color='royalblue')\n",
    "    ax1.set_xlabel('Product Rank')\n",
    "    ax1.set_ylabel('Revenue Share (%)')\n",
    "    \n",
    "    # Line chart of cumulative revenue share\n",
    "    ax2.plot(range(min(100, len(product_metrics_sorted))), \n",
    "            product_metrics_sorted['CumulativeRevenueShare'].head(100), \n",
    "            'r-', linewidth=2)\n",
    "    ax2.set_ylabel('Cumulative Revenue Share (%)')\n",
    "    ax2.set_ylim([0, 105])\n",
    "    \n",
    "    # Add 80% line\n",
    "    plt.axhline(y=80, color='green', linestyle='--', alpha=0.7)\n",
    "    plt.text(5, 82, '80% of Revenue', color='green')\n",
    "    \n",
    "    # Find how many products make up 80% of revenue\n",
    "    products_80_percent = product_metrics_sorted[product_metrics_sorted['CumulativeRevenueShare'] <= 80].shape[0]\n",
    "    \n",
    "    plt.title(f'Pareto Analysis: Revenue Distribution Across Products\\n'\n",
    "              f'Top {products_80_percent} products ({products_80_percent/len(product_metrics_sorted):.1%} of catalog) '\n",
    "              f'generate 80% of revenue')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/revenue_pareto.png', dpi=300)\n",
    "    print(\"Revenue distribution saved as 'figure/revenue_pareto.png'\")\n",
    "    \n",
    "    return product_metrics\n",
    "\n",
    "def analyze_product_performance_over_time(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Analyze how the top products perform over time.\n",
    "    \"\"\"\n",
    "    # Get top products by revenue\n",
    "    top_products = df.groupby('StockCode')['Revenue'].sum().nlargest(top_n).index.tolist()\n",
    "    \n",
    "    # Filter data for top products\n",
    "    df_top = df[df['StockCode'].isin(top_products)]\n",
    "    \n",
    "    # Group by month and product\n",
    "    monthly_product_revenue = df_top.groupby(['YearMonth', 'StockCode', 'Description'])['Revenue'].sum().reset_index()\n",
    "    \n",
    "    # Pivot data for plotting\n",
    "    pivot_data = monthly_product_revenue.pivot_table(\n",
    "        index='YearMonth', \n",
    "        columns='StockCode',\n",
    "        values='Revenue'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Create product name mapping for better labels\n",
    "    product_names = {\n",
    "        row['StockCode']: row['Description'][:20] + '...' if len(row['Description']) > 20 else row['Description']\n",
    "        for _, row in df_top[['StockCode', 'Description']].drop_duplicates().iterrows()\n",
    "    }\n",
    "    \n",
    "    # Plot time series\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    for stockcode in pivot_data.columns:\n",
    "        plt.plot(pivot_data.index, pivot_data[stockcode], marker='o', linewidth=2, label=product_names[stockcode])\n",
    "    \n",
    "    plt.title(f'Revenue Trends for Top {top_n} Products')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Revenue')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/product_revenue_trends.png', dpi=300)\n",
    "    print(\"Product revenue trends saved as 'figure/product_revenue_trends.png'\")\n",
    "    \n",
    "    # Create heatmap of monthly performance for top products\n",
    "    pivot_heatmap = monthly_product_revenue.pivot_table(\n",
    "        index='StockCode',\n",
    "        columns='YearMonth',\n",
    "        values='Revenue',\n",
    "        aggfunc='sum'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Replace stock codes with product names\n",
    "    pivot_heatmap.index = [product_names[idx] for idx in pivot_heatmap.index]\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(pivot_heatmap, cmap='viridis', annot=True, fmt='.0f', linewidths=.5)\n",
    "    plt.title('Monthly Revenue by Product')\n",
    "    plt.ylabel('Product')\n",
    "    plt.xlabel('Month')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/product_monthly_heatmap.png', dpi=300)\n",
    "    print(\"Product monthly heatmap saved as 'figure/product_monthly_heatmap.png'\")\n",
    "    \n",
    "    return monthly_product_revenue\n",
    "\n",
    "def analyze_product_by_country(df, top_n_products=10, top_n_countries=5):\n",
    "    \"\"\"\n",
    "    Analyze product performance across different countries.\n",
    "    \"\"\"\n",
    "    # Get top countries by revenue\n",
    "    top_countries = df.groupby('Country')['Revenue'].sum().nlargest(top_n_countries).index.tolist()\n",
    "    \n",
    "    # Get top products by revenue\n",
    "    top_products = df.groupby(['StockCode', 'Description'])['Revenue'].sum().nlargest(top_n_products).reset_index()\n",
    "    top_product_codes = top_products['StockCode'].tolist()\n",
    "    \n",
    "    # Filter data for top countries and products\n",
    "    df_filtered = df[\n",
    "        (df['Country'].isin(top_countries)) & \n",
    "        (df['StockCode'].isin(top_product_codes))\n",
    "    ]\n",
    "    \n",
    "    # Aggregate data by country and product\n",
    "    country_product_data = df_filtered.groupby(['Country', 'StockCode', 'Description']).agg({\n",
    "        'Revenue': 'sum',\n",
    "        'Quantity': 'sum',\n",
    "        'InvoiceNo': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Create a pivot table for the heatmap\n",
    "    pivot_data = country_product_data.pivot_table(\n",
    "        index='Country',\n",
    "        columns='StockCode',\n",
    "        values='Revenue'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Create product name mapping for better labels\n",
    "    product_names = {\n",
    "        row['StockCode']: row['Description'][:20] + '...' if len(row['Description']) > 20 else row['Description']\n",
    "        for _, row in top_products.iterrows()\n",
    "    }\n",
    "    \n",
    "    # Rename columns with product descriptions\n",
    "    pivot_data.columns = [product_names[col] for col in pivot_data.columns]\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sns.heatmap(pivot_data, annot=True, fmt='.0f', cmap='viridis')\n",
    "    plt.title(f'Product Revenue by Country (Top {top_n_products} Products, Top {top_n_countries} Countries)')\n",
    "    plt.ylabel('Country')\n",
    "    plt.xlabel('Product')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/product_country_heatmap.png', dpi=300)\n",
    "    print(\"Product by country heatmap saved as 'figure/product_country_heatmap.png'\")\n",
    "    \n",
    "    # Calculate country preference index (how much a country prefers a product vs. global average)\n",
    "    country_totals = country_product_data.groupby('Country')['Revenue'].sum().reset_index()\n",
    "    product_totals = country_product_data.groupby('StockCode')['Revenue'].sum().reset_index()\n",
    "    \n",
    "    total_revenue = country_product_data['Revenue'].sum()\n",
    "    \n",
    "    # Merge data\n",
    "    preference_data = country_product_data.merge(country_totals, on='Country', suffixes=('', '_country_total'))\n",
    "    preference_data = preference_data.merge(product_totals, on='StockCode', suffixes=('', '_product_total'))\n",
    "    \n",
    "    # Calculate expected revenue and preference index\n",
    "    preference_data['Expected_Revenue'] = (\n",
    "        preference_data['Revenue_country_total'] * preference_data['Revenue_product_total'] / total_revenue\n",
    "    )\n",
    "    preference_data['Preference_Index'] = preference_data['Revenue'] / preference_data['Expected_Revenue']\n",
    "    \n",
    "    # Create pivot table for preference index\n",
    "    pivot_preference = preference_data.pivot_table(\n",
    "        index='Country',\n",
    "        columns='StockCode',\n",
    "        values='Preference_Index'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Rename columns with product descriptions\n",
    "    pivot_preference.columns = [product_names[col] for col in pivot_preference.columns]\n",
    "    \n",
    "    # Create heatmap for preference index\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sns.heatmap(pivot_preference, annot=True, fmt='.2f', cmap='RdBu_r', center=1)\n",
    "    plt.title(f'Product Preference Index by Country (>1 = Higher than expected preference)')\n",
    "    plt.ylabel('Country')\n",
    "    plt.xlabel('Product')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/product_country_preference.png', dpi=300)\n",
    "    print(\"Product country preference index saved as 'figure/product_country_preference.png'\")\n",
    "    \n",
    "    return country_product_data, preference_data\n",
    "\n",
    "def analyze_product_performance_metrics(df):\n",
    "    \"\"\"\n",
    "    Calculate and visualize key product performance metrics.\n",
    "    \"\"\"\n",
    "    # Calculate product performance metrics\n",
    "    product_metrics = df.groupby(['StockCode', 'Description']).agg({\n",
    "        'Revenue': 'sum',\n",
    "        'Quantity': 'sum',\n",
    "        'InvoiceNo': 'nunique',\n",
    "        'CustomerID': lambda x: x.nunique() if x.notnull().any() else 0,\n",
    "        'UnitPrice': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate derived metrics\n",
    "    product_metrics['AvgOrderValue'] = product_metrics['Revenue'] / product_metrics['InvoiceNo']\n",
    "    product_metrics['AvgQuantityPerOrder'] = product_metrics['Quantity'] / product_metrics['InvoiceNo']\n",
    "    \n",
    "    # If customer ID data is available, calculate customer metrics\n",
    "    if product_metrics['CustomerID'].sum() > 0:\n",
    "        product_metrics['CustomerPenetration'] = product_metrics['CustomerID'] / df['CustomerID'].nunique()\n",
    "    \n",
    "    # Create scatter plot of quantity vs. revenue\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Use log scale for better visualization\n",
    "    plt.scatter(\n",
    "        product_metrics['Quantity'], \n",
    "        product_metrics['Revenue'],\n",
    "        alpha=0.7,\n",
    "        s=product_metrics['InvoiceNo'] / 10,  # Size points by number of transactions\n",
    "        c=product_metrics['UnitPrice'],  # Color points by unit price\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.colorbar(label='Unit Price')\n",
    "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    plt.xlabel('Quantity Sold (log scale)')\n",
    "    plt.ylabel('Revenue (log scale)')\n",
    "    plt.title('Product Performance: Quantity vs. Revenue')\n",
    "    \n",
    "    # Annotate some top products\n",
    "    top_products = product_metrics.nlargest(5, 'Revenue')\n",
    "    for _, product in top_products.iterrows():\n",
    "        plt.annotate(\n",
    "            product['Description'][:20],\n",
    "            (product['Quantity'], product['Revenue']),\n",
    "            xytext=(5, 5),\n",
    "            textcoords='offset points',\n",
    "            fontsize=8\n",
    "        )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/product_quantity_revenue.png', dpi=300)\n",
    "    print(\"Product quantity vs. revenue saved as 'figure/product_quantity_revenue.png'\")\n",
    "    \n",
    "    # Create a quadrant analysis (Boston Matrix style)\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Calculate median values for quadrant lines\n",
    "    median_revenue = product_metrics['Revenue'].median()\n",
    "    median_transactions = product_metrics['InvoiceNo'].median()\n",
    "    \n",
    "    # Create scatter plot\n",
    "    plt.scatter(\n",
    "        product_metrics['InvoiceNo'],\n",
    "        product_metrics['Revenue'],\n",
    "        s=product_metrics['Quantity'] / 100,  # Size by quantity\n",
    "        c=product_metrics['UnitPrice'],  # Color by price\n",
    "        alpha=0.7,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    \n",
    "    # Add quadrant lines\n",
    "    plt.axvline(x=median_transactions, color='gray', linestyle='--', alpha=0.7)\n",
    "    plt.axhline(y=median_revenue, color='gray', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Label quadrants\n",
    "    plt.text(max(product_metrics['InvoiceNo']) * 0.75, max(product_metrics['Revenue']) * 0.8, 'STARS', \n",
    "             fontsize=16, ha='center', bbox=dict(facecolor='yellow', alpha=0.3))\n",
    "    plt.text(min(product_metrics['InvoiceNo']) * 1.5, max(product_metrics['Revenue']) * 0.8, 'CASH COWS', \n",
    "             fontsize=16, ha='center', bbox=dict(facecolor='green', alpha=0.3))\n",
    "    plt.text(max(product_metrics['InvoiceNo']) * 0.75, min(product_metrics['Revenue']) * 2, 'QUESTION MARKS', \n",
    "             fontsize=16, ha='center', bbox=dict(facecolor='blue', alpha=0.3))\n",
    "    plt.text(min(product_metrics['InvoiceNo']) * 1.5, min(product_metrics['Revenue']) * 2, 'DOGS', \n",
    "             fontsize=16, ha='center', bbox=dict(facecolor='red', alpha=0.3))\n",
    "    \n",
    "    # Annotate top products in each quadrant\n",
    "    # Stars (high transactions, high revenue)\n",
    "    stars = product_metrics[\n",
    "        (product_metrics['InvoiceNo'] > median_transactions) & \n",
    "        (product_metrics['Revenue'] > median_revenue)\n",
    "    ].nlargest(3, 'Revenue')\n",
    "    \n",
    "    # Cash Cows (low transactions, high revenue)\n",
    "    cash_cows = product_metrics[\n",
    "        (product_metrics['InvoiceNo'] <= median_transactions) & \n",
    "        (product_metrics['Revenue'] > median_revenue)\n",
    "    ].nlargest(3, 'Revenue')\n",
    "    \n",
    "    # Question Marks (high transactions, low revenue)\n",
    "    question_marks = product_metrics[\n",
    "        (product_metrics['InvoiceNo'] > median_transactions) & \n",
    "        (product_metrics['Revenue'] <= median_revenue)\n",
    "    ].nlargest(3, 'InvoiceNo')\n",
    "    \n",
    "    # Dogs (low transactions, low revenue)\n",
    "    dogs = product_metrics[\n",
    "        (product_metrics['InvoiceNo'] <= median_transactions) & \n",
    "        (product_metrics['Revenue'] <= median_revenue)\n",
    "    ].nlargest(3, 'Revenue')\n",
    "    \n",
    "    # Annotate products\n",
    "    for _, product in pd.concat([stars, cash_cows, question_marks, dogs]).iterrows():\n",
    "        plt.annotate(\n",
    "            product['Description'][:15],\n",
    "            (product['InvoiceNo'], product['Revenue']),\n",
    "            xytext=(5, 5),\n",
    "            textcoords='offset points',\n",
    "            fontsize=8\n",
    "        )\n",
    "    \n",
    "    plt.colorbar(label='Unit Price')\n",
    "    plt.xlabel('Number of Transactions')\n",
    "    plt.ylabel('Revenue')\n",
    "    plt.title('Product Portfolio Analysis (Boston Matrix Style)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/product_portfolio_matrix.png', dpi=300)\n",
    "    print(\"Product portfolio matrix saved as 'figure/product_portfolio_matrix.png'\")\n",
    "    \n",
    "    return product_metrics\n",
    "\n",
    "def analyze_product_seasonality(df):\n",
    "    \"\"\"\n",
    "    Analyze product seasonality patterns.\n",
    "    \"\"\"\n",
    "    # Create month and day fields if not already present\n",
    "    if 'Month' not in df.columns:\n",
    "        df['Month'] = df['InvoiceDate'].dt.month\n",
    "    \n",
    "    if 'WeekDay' not in df.columns:\n",
    "        df['WeekDay'] = df['InvoiceDate'].dt.day_name()\n",
    "    \n",
    "    # Get top products\n",
    "    top_products = df.groupby(['StockCode', 'Description'])['Revenue'].sum().nlargest(10).reset_index()\n",
    "    \n",
    "    # Analyze monthly patterns for top products\n",
    "    monthly_data = df[df['StockCode'].isin(top_products['StockCode'])].groupby(\n",
    "        ['StockCode', 'Description', 'Month']\n",
    "    )['Revenue'].sum().reset_index()\n",
    "    \n",
    "    # Add month names\n",
    "    monthly_data['MonthName'] = monthly_data['Month'].apply(lambda x: calendar.month_abbr[x])\n",
    "    \n",
    "    # Create product name mapping\n",
    "    product_names = {\n",
    "        row['StockCode']: row['Description'][:20] + '...' if len(row['Description']) > 20 else row['Description']\n",
    "        for _, row in top_products.iterrows()\n",
    "    }\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(5, 2, figsize=(16, 20), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot monthly trend for each product\n",
    "    for i, stockcode in enumerate(top_products['StockCode']):\n",
    "        product_data = monthly_data[monthly_data['StockCode'] == stockcode]\n",
    "        \n",
    "        # Sort by month\n",
    "        product_data = product_data.sort_values('Month')\n",
    "        \n",
    "        # Create bar chart\n",
    "        ax = axes[i]\n",
    "        bars = ax.bar(product_data['MonthName'], product_data['Revenue'])\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                   f'£{height:.0f}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        ax.set_title(f\"{product_names[stockcode]}\")\n",
    "        ax.set_ylabel('Revenue')\n",
    "        \n",
    "        # Only add x-label for bottom plots\n",
    "        if i >= 8:\n",
    "            ax.set_xlabel('Month')\n",
    "    \n",
    "    # Remove any unused subplots\n",
    "    for j in range(i+1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/product_monthly_seasonality.png', dpi=300)\n",
    "    print(\"Product monthly seasonality saved as 'figure/product_monthly_seasonality.png'\")\n",
    "    \n",
    "    # Analyze day of week patterns\n",
    "    weekday_data = df[df['StockCode'].isin(top_products['StockCode'])].groupby(\n",
    "        ['StockCode', 'Description', 'WeekDay']\n",
    "    )['Revenue'].sum().reset_index()\n",
    "    \n",
    "    # Define weekday order\n",
    "    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    \n",
    "    # Create a heatmap for day of week patterns\n",
    "    weekday_pivot = weekday_data.pivot_table(\n",
    "        index='StockCode',\n",
    "        columns='WeekDay',\n",
    "        values='Revenue',\n",
    "        aggfunc='sum'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Reorder columns\n",
    "    weekday_pivot = weekday_pivot[\n",
    "        [day for day in weekday_order if day in weekday_pivot.columns]\n",
    "    ]\n",
    "    \n",
    "    # Replace stock codes with product names\n",
    "    weekday_pivot.index = [product_names[idx] for idx in weekday_pivot.index]\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(weekday_pivot, cmap='viridis', annot=True, fmt='.0f')\n",
    "    plt.title('Product Revenue by Day of Week')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/product_weekday_heatmap.png', dpi=300)\n",
    "    print(\"Product weekday heatmap saved as 'figure/product_weekday_heatmap.png'\")\n",
    "    \n",
    "    return monthly_data, weekday_data\n",
    "\n",
    "def create_summary_report(product_metrics, df):\n",
    "    \"\"\"\n",
    "    Create a textual summary report of key product performance insights.\n",
    "    Uses the original dataframe for transaction count calculations.\n",
    "    \"\"\"\n",
    "    # Calculate overall metrics\n",
    "    total_revenue = product_metrics['Revenue'].sum()\n",
    "    total_quantity = product_metrics['Quantity'].sum()\n",
    "    # Get total transactions from original dataframe\n",
    "    total_transactions = df['InvoiceNo'].nunique()\n",
    "    unique_products = len(product_metrics)\n",
    "    \n",
    "    # Calculate concentration metrics\n",
    "    top10_revenue = product_metrics.nlargest(10, 'Revenue')['Revenue'].sum() / total_revenue * 100\n",
    "    top20_revenue = product_metrics.nlargest(20, 'Revenue')['Revenue'].sum() / total_revenue * 100\n",
    "    top50_revenue = product_metrics.nlargest(50, 'Revenue')['Revenue'].sum() / total_revenue * 100\n",
    "    \n",
    "    # Calculate the number of products to reach 80% of revenue\n",
    "    product_metrics_sorted = product_metrics.sort_values('Revenue', ascending=False).copy()\n",
    "    product_metrics_sorted['RevenueShare'] = product_metrics_sorted['Revenue'] / total_revenue * 100\n",
    "    product_metrics_sorted['CumulativeRevenueShare'] = product_metrics_sorted['RevenueShare'].cumsum()\n",
    "    products_80_percent = product_metrics_sorted[product_metrics_sorted['CumulativeRevenueShare'] <= 80].shape[0]\n",
    "    \n",
    "    # Revenue per product statistics\n",
    "    avg_revenue_per_product = total_revenue / unique_products\n",
    "    median_revenue_per_product = product_metrics['Revenue'].median()\n",
    "    \n",
    "    # Create the report\n",
    "    report = f\"\"\"\n",
    "    PRODUCT PERFORMANCE SUMMARY REPORT\n",
    "    ==================================\n",
    "    \n",
    "    OVERALL METRICS:\n",
    "    - Total Revenue: £{total_revenue:,.2f}\n",
    "    - Total Quantity Sold: {total_quantity:,}\n",
    "    - Total Transactions: {total_transactions:,}\n",
    "    - Unique Products: {unique_products:,}\n",
    "    \n",
    "    PRODUCT CONCENTRATION:\n",
    "    - Top 10 Products: {top10_revenue:.1f}% of total revenue\n",
    "    - Top 20 Products: {top20_revenue:.1f}% of total revenue\n",
    "    - Top 50 Products: {top50_revenue:.1f}% of total revenue\n",
    "    - Number of products to reach 80% of revenue: {products_80_percent:,} ({products_80_percent/unique_products:.1%} of catalog)\n",
    "    \n",
    "    REVENUE PER PRODUCT:\n",
    "    - Average Revenue per Product: £{avg_revenue_per_product:,.2f}\n",
    "    - Median Revenue per Product: £{median_revenue_per_product:,.2f}\n",
    "    \n",
    "    TOP 3 PRODUCTS BY REVENUE:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add top 3 products\n",
    "    for i, (_, product) in enumerate(product_metrics.nlargest(3, 'Revenue').iterrows(), 1):\n",
    "        report += f\"\"\"\n",
    "    {i}. {product['Description']}\n",
    "       - Revenue: £{product['Revenue']:,.2f} ({product['Revenue']/total_revenue:.1%} of total)\n",
    "       - Quantity: {product['Quantity']:,} units\n",
    "       - Transactions: {product['InvoiceNo']:,}\n",
    "       - Unit Price: £{product['UnitPrice']:,.2f}\n",
    "        \"\"\"\n",
    "    \n",
    "    # Add recommendations section\n",
    "    report += \"\"\"\n",
    "    RECOMMENDATIONS:\n",
    "    1. Focus marketing efforts on top revenue-generating products\n",
    "    2. Review the long tail of slow-moving products for potential discontinuation\n",
    "    3. Analyze seasonal patterns to optimize inventory planning\n",
    "    4. Investigate cross-selling opportunities between top products\n",
    "    5. Consider country-specific promotions based on product preferences\n",
    "    \"\"\"\n",
    "    \n",
    "    # Print report\n",
    "    print(report)\n",
    "    \n",
    "    # Save report to file\n",
    "    with open('figure/product_performance_report.txt', 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(\"Product performance report saved as 'figure/product_performance_report.txt'\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "def run_product_performance_analysis(file_path):\n",
    "    \"\"\"\n",
    "    Run the complete product performance analysis workflow.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"PRODUCT PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    df = load_and_prepare_data(file_path)\n",
    "    \n",
    "    # Create figure directory if it doesn't exist\n",
    "    if not os.path.exists('figure'):\n",
    "        os.makedirs('figure')\n",
    "    \n",
    "    # Analyze top products\n",
    "    product_metrics = analyze_top_products(df, n=20)\n",
    "    \n",
    "    # Analyze product performance over time\n",
    "    monthly_product_data = analyze_product_performance_over_time(df, top_n=10)\n",
    "    \n",
    "    # Analyze product by country\n",
    "    country_product_data, preference_data = analyze_product_by_country(df, top_n_products=10, top_n_countries=5)\n",
    "    \n",
    "    # Analyze product performance metrics\n",
    "    product_metrics_detailed = analyze_product_performance_metrics(df)\n",
    "    \n",
    "    # Analyze product seasonality\n",
    "    monthly_data, weekday_data = analyze_product_seasonality(df)\n",
    "    \n",
    "    # Create summary report\n",
    "    summary_report = create_summary_report(product_metrics, df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Return analysis components for further exploration\n",
    "    return {\n",
    "        'product_metrics': product_metrics,\n",
    "        'monthly_product_data': monthly_product_data,\n",
    "        'country_product_data': country_product_data,\n",
    "        'preference_data': preference_data,\n",
    "        'product_metrics_detailed': product_metrics_detailed,\n",
    "        'monthly_data': monthly_data,\n",
    "        'weekday_data': weekday_data\n",
    "    }\n",
    "\n",
    "# If running as a script\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the analysis\n",
    "    file_path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx\"  # Update with your file path\n",
    "    results = run_product_performance_analysis(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Predictive Revenue - XGBoost Regression (machine-learning-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import holidays\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_theme()\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "def load_and_prepare_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and prepare the retail dataset for demand forecasting.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    print(\"\\nDataset Overview:\")\n",
    "    print(f\"- Shape: {df.shape}\")\n",
    "    print(f\"- Columns: {', '.join(df.columns)}\")\n",
    "    \n",
    "    # Basic data cleaning\n",
    "    # Remove canceled orders (negative quantities)\n",
    "    df_cleaned = df[df['Quantity'] > 0]\n",
    "    \n",
    "    # Remove missing values\n",
    "    df_cleaned = df_cleaned.dropna(subset=['StockCode', 'Description'])\n",
    "    \n",
    "    # Filter out non-product items\n",
    "    non_product_codes = ['POST', 'D', 'M', 'CRUK', 'C2', 'DOT', 'BANK CHARGES']\n",
    "    df_cleaned = df_cleaned[~df_cleaned['StockCode'].isin(non_product_codes)]\n",
    "    \n",
    "    # Filter out items with generic descriptions\n",
    "    generic_descriptions = ['DISCOUNT', 'POSTAGE', 'MANUAL', 'SAMPLES', 'TEST', 'ADJUST']\n",
    "    pattern = '|'.join(generic_descriptions)\n",
    "    df_cleaned = df_cleaned[~df_cleaned['Description'].str.contains(pattern, case=False, na=False)]\n",
    "    \n",
    "    # Add total value column\n",
    "    df_cleaned['Revenue'] = df_cleaned['Quantity'] * df_cleaned['UnitPrice']\n",
    "    \n",
    "    # Convert InvoiceDate to datetime if it's not already\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_cleaned['InvoiceDate']):\n",
    "        df_cleaned['InvoiceDate'] = pd.to_datetime(df_cleaned['InvoiceDate'])\n",
    "    \n",
    "    # Extract date components\n",
    "    df_cleaned['Date'] = df_cleaned['InvoiceDate'].dt.date\n",
    "    df_cleaned['Year'] = df_cleaned['InvoiceDate'].dt.year\n",
    "    df_cleaned['Month'] = df_cleaned['InvoiceDate'].dt.month\n",
    "    df_cleaned['Day'] = df_cleaned['InvoiceDate'].dt.day\n",
    "    df_cleaned['WeekDay'] = df_cleaned['InvoiceDate'].dt.day_name()\n",
    "    df_cleaned['DayOfWeek'] = df_cleaned['InvoiceDate'].dt.dayofweek\n",
    "    df_cleaned['Hour'] = df_cleaned['InvoiceDate'].dt.hour\n",
    "    df_cleaned['WeekOfYear'] = df_cleaned['InvoiceDate'].dt.isocalendar().week\n",
    "    df_cleaned['DayOfYear'] = df_cleaned['InvoiceDate'].dt.dayofyear\n",
    "    df_cleaned['Quarter'] = df_cleaned['InvoiceDate'].dt.quarter\n",
    "    \n",
    "    # Create a flag for weekend vs weekday\n",
    "    df_cleaned['IsWeekend'] = df_cleaned['DayOfWeek'].isin([5, 6]).astype(int)  # 5=Sat, 6=Sun\n",
    "    \n",
    "    # Add UK holidays\n",
    "    uk_holidays = holidays.UK()\n",
    "    df_cleaned['IsHoliday'] = df_cleaned['Date'].apply(lambda x: x in uk_holidays).astype(int)\n",
    "    \n",
    "    print(f\"\\nAfter cleaning: {df_cleaned.shape[0]} transactions\")\n",
    "    return df_cleaned\n",
    "\n",
    "def prepare_time_series_data(df, freq='D', min_transactions=100):\n",
    "    \"\"\"\n",
    "    Aggregate data for time series analysis at specified frequency.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Input DataFrame with transaction data\n",
    "    - freq: Frequency for aggregation ('D' for daily, 'W' for weekly, 'M' for monthly)\n",
    "    - min_transactions: Minimum number of transactions for a product to be included\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary of DataFrames for overall demand and top products\n",
    "    \"\"\"\n",
    "    print(f\"\\nPreparing time series data with {freq} frequency...\")\n",
    "    \n",
    "    # Convert dates for proper aggregation\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Determine time period based on frequency\n",
    "    if freq == 'D':\n",
    "        time_col = 'Date'\n",
    "        df['TimePeriod'] = df['Date']\n",
    "    elif freq == 'W':\n",
    "        time_col = 'Week'\n",
    "        df['TimePeriod'] = df['Date'] - pd.to_timedelta(df['Date'].dt.dayofweek, unit='d')  # Start of week\n",
    "    elif freq == 'M':\n",
    "        time_col = 'Month'\n",
    "        df['TimePeriod'] = df['Date'] - pd.to_timedelta(df['Date'].dt.day - 1, unit='d')  # Start of month\n",
    "    \n",
    "    # Get overall demand by time period\n",
    "    overall_demand = df.groupby('TimePeriod').agg({\n",
    "        'Quantity': 'sum',\n",
    "        'Revenue': 'sum',\n",
    "        'InvoiceNo': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Add date components back to the aggregate data\n",
    "    overall_demand['Year'] = overall_demand['TimePeriod'].dt.year\n",
    "    overall_demand['Month'] = overall_demand['TimePeriod'].dt.month\n",
    "    overall_demand['Day'] = overall_demand['TimePeriod'].dt.day\n",
    "    overall_demand['DayOfWeek'] = overall_demand['TimePeriod'].dt.dayofweek\n",
    "    overall_demand['DayOfYear'] = overall_demand['TimePeriod'].dt.dayofyear\n",
    "    overall_demand['WeekOfYear'] = overall_demand['TimePeriod'].dt.isocalendar().week\n",
    "    overall_demand['Quarter'] = overall_demand['TimePeriod'].dt.quarter\n",
    "    overall_demand['IsWeekend'] = overall_demand['DayOfWeek'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Add UK holidays\n",
    "    uk_holidays = holidays.UK()\n",
    "    overall_demand['IsHoliday'] = overall_demand['TimePeriod'].apply(lambda x: x in uk_holidays).astype(int)\n",
    "    \n",
    "    # Find products with enough transactions for individual forecasting\n",
    "    product_transaction_counts = df.groupby('StockCode').size()\n",
    "    top_products = product_transaction_counts[product_transaction_counts >= min_transactions].index.tolist()\n",
    "    \n",
    "    print(f\"Found {len(top_products)} products with at least {min_transactions} transactions\")\n",
    "    \n",
    "    # Get top 3 products by revenue for individual forecasting\n",
    "    if len(top_products) > 3:\n",
    "        product_revenue = df[df['StockCode'].isin(top_products)].groupby('StockCode')['Revenue'].sum()\n",
    "        top_products = product_revenue.nlargest(3).index.tolist()\n",
    "    \n",
    "    # Create demand data for each top product\n",
    "    product_demand = {}\n",
    "    for stock_code in top_products:\n",
    "        # Filter data for this product\n",
    "        product_df = df[df['StockCode'] == stock_code]\n",
    "        \n",
    "        # Get product description\n",
    "        product_description = product_df['Description'].iloc[0]\n",
    "        \n",
    "        # Aggregate by time period\n",
    "        product_ts = product_df.groupby('TimePeriod').agg({\n",
    "            'Quantity': 'sum',\n",
    "            'Revenue': 'sum',\n",
    "            'InvoiceNo': 'nunique'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Add date components\n",
    "        product_ts['Year'] = product_ts['TimePeriod'].dt.year\n",
    "        product_ts['Month'] = product_ts['TimePeriod'].dt.month\n",
    "        product_ts['Day'] = product_ts['TimePeriod'].dt.day\n",
    "        product_ts['DayOfWeek'] = product_ts['TimePeriod'].dt.dayofweek\n",
    "        product_ts['DayOfYear'] = product_ts['TimePeriod'].dt.dayofyear\n",
    "        product_ts['WeekOfYear'] = product_ts['TimePeriod'].dt.isocalendar().week\n",
    "        product_ts['Quarter'] = product_ts['TimePeriod'].dt.quarter\n",
    "        product_ts['IsWeekend'] = product_ts['DayOfWeek'].isin([5, 6]).astype(int)\n",
    "        product_ts['IsHoliday'] = product_ts['TimePeriod'].apply(lambda x: x in uk_holidays).astype(int)\n",
    "        \n",
    "        # Store in dictionary\n",
    "        product_demand[stock_code] = {\n",
    "            'description': product_description,\n",
    "            'data': product_ts\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'overall_demand': overall_demand,\n",
    "        'product_demand': product_demand,\n",
    "        'top_products': top_products\n",
    "    }\n",
    "\n",
    "def add_lag_features(df, target_col, lag_periods):\n",
    "    \"\"\"\n",
    "    Add lagged features for time series forecasting.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Time series DataFrame\n",
    "    - target_col: Target column to create lags for\n",
    "    - lag_periods: List of periods to lag\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with lag features added\n",
    "    \"\"\"\n",
    "    df_with_lags = df.copy()\n",
    "    \n",
    "    for lag in lag_periods:\n",
    "        lag_col_name = f'{target_col}_lag_{lag}'\n",
    "        df_with_lags[lag_col_name] = df_with_lags[target_col].shift(lag)\n",
    "    \n",
    "    # Drop rows with NaN values from lag features\n",
    "    df_with_lags = df_with_lags.dropna()\n",
    "    \n",
    "    return df_with_lags\n",
    "\n",
    "def add_rolling_features(df, target_col, windows):\n",
    "    \"\"\"\n",
    "    Add rolling window features for time series forecasting.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Time series DataFrame\n",
    "    - target_col: Target column to create features for\n",
    "    - windows: List of window sizes\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with rolling features added\n",
    "    \"\"\"\n",
    "    df_with_rolling = df.copy()\n",
    "    \n",
    "    for window in windows:\n",
    "        # Add rolling mean\n",
    "        df_with_rolling[f'{target_col}_rolling_mean_{window}'] = df_with_rolling[target_col].rolling(window=window).mean()\n",
    "        \n",
    "        # Add rolling standard deviation\n",
    "        df_with_rolling[f'{target_col}_rolling_std_{window}'] = df_with_rolling[target_col].rolling(window=window).std()\n",
    "        \n",
    "        # Add rolling min/max\n",
    "        df_with_rolling[f'{target_col}_rolling_min_{window}'] = df_with_rolling[target_col].rolling(window=window).min()\n",
    "        df_with_rolling[f'{target_col}_rolling_max_{window}'] = df_with_rolling[target_col].rolling(window=window).max()\n",
    "    \n",
    "    # Drop rows with NaN values from rolling features\n",
    "    df_with_rolling = df_with_rolling.dropna()\n",
    "    \n",
    "    return df_with_rolling\n",
    "\n",
    "def train_overall_demand_forecast_model(time_series_data, target_col='Quantity', horizon=7):\n",
    "    \"\"\"\n",
    "    Train a machine learning model to forecast overall demand.\n",
    "    \n",
    "    Parameters:\n",
    "    - time_series_data: Dictionary with time series data\n",
    "    - target_col: Target column to forecast ('Quantity' or 'Revenue')\n",
    "    - horizon: Number of time periods to forecast\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with model, predictions, and evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nTraining overall demand forecast model for {target_col}...\")\n",
    "    \n",
    "    # Get data\n",
    "    overall_demand = time_series_data['overall_demand'].copy()\n",
    "    \n",
    "    # Sort by time period\n",
    "    overall_demand = overall_demand.sort_values('TimePeriod')\n",
    "    \n",
    "    # Add lag features\n",
    "    lag_periods = [1, 2, 3, 7]  # Use fewer lags to avoid errors\n",
    "    df_with_lags = add_lag_features(overall_demand, target_col, lag_periods)\n",
    "    \n",
    "    # Add rolling features\n",
    "    windows = [7, 14]  # Week, 2 weeks\n",
    "    df_with_features = add_rolling_features(df_with_lags, target_col, windows)\n",
    "    \n",
    "    # Prepare features and target\n",
    "    feature_cols = [col for col in df_with_features.columns if col != target_col and \n",
    "                  col not in ['TimePeriod', 'InvoiceNo']]\n",
    "    \n",
    "    X = df_with_features[feature_cols]\n",
    "    y = df_with_features[target_col]\n",
    "    \n",
    "    # Handle missing values - Fill NaN with appropriate values\n",
    "    X = X.fillna(0)  # Fill missing feature values with 0\n",
    "    \n",
    "    # Split data into training and testing sets (time-based split)\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "    y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "    \n",
    "    # Create and train models\n",
    "    models = {\n",
    "        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        'Ridge': Ridge(alpha=1.0, random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Add XGBoost and LightGBM if available\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "        models['XGBoost'] = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "    except ImportError:\n",
    "        print(\"XGBoost not available. Skipping XGBoost model.\")\n",
    "    \n",
    "    try:\n",
    "        import lightgbm as lgb\n",
    "        models['LightGBM'] = lgb.LGBMRegressor(n_estimators=100, random_state=42)\n",
    "    except ImportError:\n",
    "        print(\"LightGBM not available. Skipping LightGBM model.\")\n",
    "    \n",
    "    best_model_name = None\n",
    "    best_model = None\n",
    "    best_score = float('inf')  # Lower is better for MAE\n",
    "    \n",
    "    model_results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate model\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"{name} Model Evaluation:\")\n",
    "        print(f\"- MAE: {mae:.2f}\")\n",
    "        print(f\"- RMSE: {rmse:.2f}\")\n",
    "        print(f\"- R²: {r2:.4f}\")\n",
    "        \n",
    "        model_results[name] = {\n",
    "            'model': model,\n",
    "            'predictions': y_pred,\n",
    "            'metrics': {\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'r2': r2\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Update best model\n",
    "        if mae < best_score:\n",
    "            best_score = mae\n",
    "            best_model_name = name\n",
    "            best_model = model\n",
    "    \n",
    "    print(f\"\\nBest model: {best_model_name} with MAE = {best_score:.2f}\")\n",
    "    \n",
    "    # Get feature importance for the best model if available\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_cols,\n",
    "            'Importance': best_model.feature_importances_\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Visualize feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.barh(feature_importance['Feature'][:15], feature_importance['Importance'][:15])\n",
    "        plt.gca().invert_yaxis()  # Invert y-axis to show highest importance at the top\n",
    "        plt.title(f'Feature Importance for {target_col} Forecast')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'figure/feature_importance_{target_col.lower()}.png', dpi=300)\n",
    "        \n",
    "        model_results['feature_importance'] = feature_importance\n",
    "    \n",
    "    # Visualize actual vs predicted values\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Get time periods for the test set\n",
    "    test_time_periods = df_with_features.iloc[train_size:]['TimePeriod'].values\n",
    "    \n",
    "    plt.plot(test_time_periods, y_test.values, marker='o', linestyle='-', label='Actual')\n",
    "    plt.plot(test_time_periods, model_results[best_model_name]['predictions'], marker='x', linestyle='--', label='Predicted')\n",
    "    \n",
    "    plt.title(f'Actual vs Predicted {target_col} (using {best_model_name})')\n",
    "    plt.xlabel('Time Period')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figure/actual_vs_predicted_{target_col.lower()}.png', dpi=300)\n",
    "    \n",
    "    # Create a forecast for the next 'horizon' periods\n",
    "    latest_data = df_with_features.iloc[-1:].copy()\n",
    "    forecasts = []\n",
    "    \n",
    "    for i in range(horizon):\n",
    "        # Make a copy to avoid modifying the original\n",
    "        forecast_features = latest_data.copy()\n",
    "        \n",
    "        # Ensure no missing values in forecast features\n",
    "        feature_data = forecast_features[feature_cols].fillna(0)\n",
    "        \n",
    "        # Make prediction using the current features\n",
    "        forecast = best_model.predict(feature_data)\n",
    "        forecast_value = forecast[0]\n",
    "        \n",
    "        # Store forecast\n",
    "        next_date = latest_data['TimePeriod'].iloc[0] + pd.Timedelta(days=1)\n",
    "        forecasts.append({\n",
    "            'TimePeriod': next_date,\n",
    "            f'{target_col}_Forecast': forecast_value\n",
    "        })\n",
    "        \n",
    "        # Create a new row for the next step\n",
    "        new_row = pd.DataFrame({\n",
    "            'TimePeriod': [next_date],\n",
    "            target_col: [forecast_value]\n",
    "        })\n",
    "        \n",
    "        # Add date features for the new row\n",
    "        new_row['Year'] = new_row['TimePeriod'].dt.year\n",
    "        new_row['Month'] = new_row['TimePeriod'].dt.month\n",
    "        new_row['Day'] = new_row['TimePeriod'].dt.day\n",
    "        new_row['DayOfWeek'] = new_row['TimePeriod'].dt.dayofweek\n",
    "        new_row['DayOfYear'] = new_row['TimePeriod'].dt.dayofyear\n",
    "        new_row['WeekOfYear'] = new_row['TimePeriod'].dt.isocalendar().week\n",
    "        new_row['Quarter'] = (new_row['TimePeriod'].dt.month - 1) // 3 + 1\n",
    "        new_row['IsWeekend'] = (new_row['DayOfWeek'] >= 5).astype(int)\n",
    "        \n",
    "        # Add UK holidays\n",
    "        uk_holidays = holidays.UK()\n",
    "        new_row['IsHoliday'] = new_row['TimePeriod'].dt.date.apply(lambda x: x in uk_holidays).astype(int)\n",
    "        \n",
    "        # Create a full dataframe by appending the new row to the history\n",
    "        temp_df = pd.concat([df_with_features, new_row], ignore_index=True)\n",
    "        \n",
    "        # Recalculate lag features\n",
    "        for lag in lag_periods:\n",
    "            temp_df[f'{target_col}_lag_{lag}'] = temp_df[target_col].shift(lag)\n",
    "        \n",
    "        # Recalculate rolling features\n",
    "        for window in windows:\n",
    "            temp_df[f'{target_col}_rolling_mean_{window}'] = temp_df[target_col].rolling(window=window).mean()\n",
    "            temp_df[f'{target_col}_rolling_std_{window}'] = temp_df[target_col].rolling(window=window).std()\n",
    "            temp_df[f'{target_col}_rolling_min_{window}'] = temp_df[target_col].rolling(window=window).min()\n",
    "            temp_df[f'{target_col}_rolling_max_{window}'] = temp_df[target_col].rolling(window=window).max()\n",
    "        \n",
    "        # Update latest_data to be the last row (which has all the features calculated)\n",
    "        latest_data = temp_df.iloc[[-1]]\n",
    "    \n",
    "    # Create a DataFrame with forecasts\n",
    "    forecast_df = pd.DataFrame(forecasts)\n",
    "    \n",
    "    # Visualize forecasts\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Get historical data for context\n",
    "    historical_periods = df_with_features['TimePeriod'].values[-30:]  # Last 30 days\n",
    "    historical_values = df_with_features[target_col].values[-30:]\n",
    "    \n",
    "    # Plot historical and forecast\n",
    "    plt.plot(historical_periods, historical_values, marker='o', linestyle='-', label='Historical')\n",
    "    plt.plot(forecast_df['TimePeriod'], forecast_df[f'{target_col}_Forecast'], marker='x', linestyle='--', color='red', label='Forecast')\n",
    "    \n",
    "    plt.title(f'{target_col} Forecast for Next {horizon} Days')\n",
    "    plt.xlabel('Time Period')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figure/forecast_{target_col.lower()}.png', dpi=300)\n",
    "    \n",
    "    model_results['best_model_name'] = best_model_name\n",
    "    model_results['forecasts'] = forecast_df\n",
    "    \n",
    "    return model_results\n",
    "\n",
    "def train_product_demand_forecast_model(time_series_data, product_code, target_col='Quantity', horizon=7):\n",
    "    \"\"\"\n",
    "    Train a machine learning model to forecast demand for a specific product.\n",
    "    \n",
    "    Parameters:\n",
    "    - time_series_data: Dictionary with time series data\n",
    "    - product_code: Stock code of the product to forecast\n",
    "    - target_col: Target column to forecast ('Quantity' or 'Revenue')\n",
    "    - horizon: Number of time periods to forecast\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with model, predictions, and evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nTraining demand forecast model for product {product_code} ({target_col})...\")\n",
    "    \n",
    "    try:\n",
    "        # Get product data\n",
    "        product_data = time_series_data['product_demand'][product_code]\n",
    "        description = product_data['description']\n",
    "        df = product_data['data'].copy()\n",
    "        \n",
    "        # Sort by time period\n",
    "        df = df.sort_values('TimePeriod')\n",
    "        \n",
    "        # Check if we have enough data\n",
    "        if len(df) < 30:  # Arbitrary threshold\n",
    "            print(f\"Warning: Limited data for product {product_code} ({len(df)} data points). Forecast may be unreliable.\")\n",
    "        \n",
    "        # Add lag features\n",
    "        lag_periods = [1, 2, 3]  # Use fewer lags if data is limited\n",
    "        df_with_lags = add_lag_features(df, target_col, lag_periods)\n",
    "        \n",
    "        # Add rolling features if we have enough data\n",
    "        if len(df) >= 14:\n",
    "            windows = [7, 14]  # Week, 2 weeks\n",
    "            df_with_features = add_rolling_features(df_with_lags, target_col, windows)\n",
    "        else:\n",
    "            df_with_features = df_with_lags\n",
    "        \n",
    "        # Prepare features and target\n",
    "        feature_cols = [col for col in df_with_features.columns if col != target_col and \n",
    "                      col not in ['TimePeriod', 'InvoiceNo']]\n",
    "        \n",
    "        # Handle missing values - Fill NaN with appropriate values\n",
    "        df_with_features = df_with_features.fillna(0)\n",
    "        \n",
    "        X = df_with_features[feature_cols]\n",
    "        y = df_with_features[target_col]\n",
    "        \n",
    "        # Split data into training and testing sets (time-based split)\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        if train_size < 1:\n",
    "            # Not enough data for train/test split\n",
    "            print(f\"Error: Not enough data for product {product_code} after feature creation.\")\n",
    "            return None\n",
    "        \n",
    "        X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "        y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "        \n",
    "        # Train a model\n",
    "        # Use a simpler model if data is limited\n",
    "        if len(df) < 50:\n",
    "            model = Ridge(alpha=1.0, random_state=42)\n",
    "        else:\n",
    "            model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate model\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"Model Evaluation for {description[:30]}...\")\n",
    "        print(f\"- MAE: {mae:.2f}\")\n",
    "        print(f\"- RMSE: {rmse:.2f}\")\n",
    "        print(f\"- R²: {r2:.4f}\")\n",
    "        \n",
    "        # Visualize actual vs predicted values\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        \n",
    "        # Get time periods for the test set\n",
    "        test_time_periods = df_with_features.iloc[train_size:]['TimePeriod'].values\n",
    "        \n",
    "        plt.plot(test_time_periods, y_test.values, marker='o', linestyle='-', label='Actual')\n",
    "        plt.plot(test_time_periods, y_pred, marker='x', linestyle='--', label='Predicted')\n",
    "        \n",
    "        plt.title(f'Actual vs Predicted {target_col} for {description[:30]}...')\n",
    "        plt.xlabel('Time Period')\n",
    "        plt.ylabel(target_col)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'figure/product_{product_code}_{target_col.lower()}_prediction.png', dpi=300)\n",
    "        \n",
    "        # Create a forecast for the next 'horizon' periods\n",
    "        latest_data = df_with_features.iloc[-1:].copy()\n",
    "        forecasts = []\n",
    "        \n",
    "        for i in range(horizon):\n",
    "            # Make a copy to avoid modifying the original\n",
    "            forecast_features = latest_data.copy()\n",
    "            \n",
    "            # Ensure no missing values in forecast features\n",
    "            feature_data = forecast_features[feature_cols].fillna(0)\n",
    "            \n",
    "            # Make prediction using the current features\n",
    "            forecast = model.predict(feature_data)\n",
    "            forecast_value = forecast[0]\n",
    "            \n",
    "            # Store forecast\n",
    "            next_date = latest_data['TimePeriod'].iloc[0] + pd.Timedelta(days=1)\n",
    "            forecasts.append({\n",
    "                'TimePeriod': next_date,\n",
    "                f'{target_col}_Forecast': forecast_value\n",
    "            })\n",
    "            \n",
    "            # Create a new row for the next step\n",
    "            new_row = pd.DataFrame({\n",
    "                'TimePeriod': [next_date],\n",
    "                target_col: [forecast_value]\n",
    "            })\n",
    "            \n",
    "            # Add date features for the new row\n",
    "            new_row['Year'] = new_row['TimePeriod'].dt.year\n",
    "            new_row['Month'] = new_row['TimePeriod'].dt.month\n",
    "            new_row['Day'] = new_row['TimePeriod'].dt.day\n",
    "            new_row['DayOfWeek'] = new_row['TimePeriod'].dt.dayofweek\n",
    "            new_row['DayOfYear'] = new_row['TimePeriod'].dt.dayofyear\n",
    "            new_row['WeekOfYear'] = new_row['TimePeriod'].dt.isocalendar().week\n",
    "            new_row['Quarter'] = (new_row['TimePeriod'].dt.month - 1) // 3 + 1\n",
    "            new_row['IsWeekend'] = (new_row['DayOfWeek'] >= 5).astype(int)\n",
    "            \n",
    "            # Add UK holidays\n",
    "            uk_holidays = holidays.UK()\n",
    "            new_row['IsHoliday'] = new_row['TimePeriod'].dt.date.apply(lambda x: x in uk_holidays).astype(int)\n",
    "            \n",
    "            # Create a full dataframe by appending the new row to the history\n",
    "            temp_df = pd.concat([df_with_features, new_row], ignore_index=True)\n",
    "            \n",
    "            # Recalculate lag features\n",
    "            for lag in lag_periods:\n",
    "                temp_df[f'{target_col}_lag_{lag}'] = temp_df[target_col].shift(lag)\n",
    "            \n",
    "            # Recalculate rolling features if they exist\n",
    "            if len(df) >= 14:\n",
    "                for window in windows:\n",
    "                    temp_df[f'{target_col}_rolling_mean_{window}'] = temp_df[target_col].rolling(window=window).mean()\n",
    "                    temp_df[f'{target_col}_rolling_std_{window}'] = temp_df[target_col].rolling(window=window).std()\n",
    "                    temp_df[f'{target_col}_rolling_min_{window}'] = temp_df[target_col].rolling(window=window).min()\n",
    "                    temp_df[f'{target_col}_rolling_max_{window}'] = temp_df[target_col].rolling(window=window).max()\n",
    "            \n",
    "            # Fill any NaNs that might have been created\n",
    "            temp_df = temp_df.fillna(0)\n",
    "            \n",
    "            # Update latest_data to be the last row (which has all the features calculated)\n",
    "            latest_data = temp_df.iloc[[-1]]\n",
    "        \n",
    "        # Create a DataFrame with forecasts\n",
    "        forecast_df = pd.DataFrame(forecasts)\n",
    "        \n",
    "        # Visualize forecasts\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        \n",
    "        # Get historical data for context\n",
    "        history_size = min(30, len(df_with_features))  # Last 30 days or all available\n",
    "        historical_periods = df_with_features['TimePeriod'].values[-history_size:]\n",
    "        historical_values = df_with_features[target_col].values[-history_size:]\n",
    "        \n",
    "        # Plot historical and forecast\n",
    "        plt.plot(historical_periods, historical_values, marker='o', linestyle='-', label='Historical')\n",
    "        plt.plot(forecast_df['TimePeriod'], forecast_df[f'{target_col}_Forecast'], marker='x', linestyle='--', color='red', label='Forecast')\n",
    "        \n",
    "        plt.title(f'{target_col} Forecast for {description[:30]}... (Next {horizon} Days)')\n",
    "        plt.xlabel('Time Period')\n",
    "        plt.ylabel(target_col)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'figure/product_{product_code}_{target_col.lower()}_forecast.png', dpi=300)\n",
    "        \n",
    "        # Return results\n",
    "        return {\n",
    "            'product_code': product_code,\n",
    "            'description': description,\n",
    "            'model': model,\n",
    "            'predictions': y_pred,\n",
    "            'metrics': {\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'r2': r2\n",
    "            },\n",
    "            'forecasts': forecast_df\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Enhanced error handling\n",
    "        print(f\"Error processing product {product_code}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_combined_forecast_visualization(overall_forecast, product_forecasts, target_col='Quantity'):\n",
    "    \"\"\"\n",
    "    Create a combined visualization of overall and product-specific forecasts.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Plot overall forecast\n",
    "    overall_history = overall_forecast['historical_data']\n",
    "    overall_forecast_data = overall_forecast['forecasts']\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(overall_history['TimePeriod'], overall_history[target_col], marker='o', linestyle='-', label='Historical')\n",
    "    plt.plot(overall_forecast_data['TimePeriod'], overall_forecast_data[f'{target_col}_Forecast'], \n",
    "            marker='x', linestyle='--', color='red', label='Forecast')\n",
    "    \n",
    "    plt.title(f'Overall {target_col} Forecast')\n",
    "    plt.xlabel('Time Period')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Plot product-specific forecasts\n",
    "    plt.subplot(2, 1, 2)\n",
    "    \n",
    "    # Select top 3 products by forecast volume\n",
    "    top_products = []\n",
    "    for product_code, forecast_data in product_forecasts.items():\n",
    "        avg_forecast = forecast_data['forecasts'][f'{target_col}_Forecast'].mean()\n",
    "        top_products.append((product_code, forecast_data['description'], avg_forecast))\n",
    "    \n",
    "    top_products.sort(key=lambda x: x[2], reverse=True)\n",
    "    top_products = top_products[:5]  # Take top 5\n",
    "    \n",
    "    for product_code, description, _ in top_products:\n",
    "        forecast_data = product_forecasts[product_code]\n",
    "        forecasts = forecast_data['forecasts']\n",
    "        plt.plot(forecasts['TimePeriod'], forecasts[f'{target_col}_Forecast'], \n",
    "                marker='o', linestyle='-', label=f\"{description[:20]}...\")\n",
    "    \n",
    "    plt.title(f'Top 3 Product {target_col} Forecasts')\n",
    "    plt.xlabel('Time Period')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figure/combined_{target_col.lower()}_forecast.png', dpi=300)\n",
    "    print(f\"Combined {target_col} forecast visualization saved as 'figure/combined_{target_col.lower()}_forecast.png'\")\n",
    "\n",
    "def create_forecast_summary_report(overall_forecast, product_forecasts, target_col='Quantity', horizon=7):\n",
    "    \"\"\"\n",
    "    Create a text report summarizing the forecasting results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get overall forecast data - with error handling for missing keys\n",
    "        best_model_name = overall_forecast.get('best_model_name', 'Unknown')\n",
    "        overall_forecast_data = overall_forecast.get('forecasts', pd.DataFrame())\n",
    "        \n",
    "        # Get metrics from the best model if available\n",
    "        overall_metrics = {}\n",
    "        if best_model_name and best_model_name in overall_forecast:\n",
    "            overall_metrics = overall_forecast[best_model_name].get('metrics', {})\n",
    "        else:\n",
    "            # Try to find any metrics\n",
    "            for model_name in overall_forecast:\n",
    "                if isinstance(overall_forecast[model_name], dict) and 'metrics' in overall_forecast[model_name]:\n",
    "                    overall_metrics = overall_forecast[model_name]['metrics']\n",
    "                    best_model_name = model_name\n",
    "                    break\n",
    "        \n",
    "        # Default values if metrics not found\n",
    "        mae = overall_metrics.get('mae', 'N/A')\n",
    "        rmse = overall_metrics.get('rmse', 'N/A')\n",
    "        r2 = overall_metrics.get('r2', 'N/A')\n",
    "        \n",
    "        # Calculate overall forecast summary with error handling\n",
    "        if not overall_forecast_data.empty and f'{target_col}_Forecast' in overall_forecast_data.columns:\n",
    "            total_forecast = overall_forecast_data[f'{target_col}_Forecast'].sum()\n",
    "            avg_daily_forecast = overall_forecast_data[f'{target_col}_Forecast'].mean()\n",
    "        else:\n",
    "            total_forecast = \"N/A\"\n",
    "            avg_daily_forecast = \"N/A\"\n",
    "        \n",
    "        # Prepare the report\n",
    "        report = f\"\"\"\n",
    "        DEMAND FORECAST SUMMARY REPORT\n",
    "        ==============================\n",
    "        \n",
    "        FORECAST PERIOD: Next {horizon} days\n",
    "        TARGET METRIC: {target_col}\n",
    "        \n",
    "        OVERALL FORECAST:\n",
    "        - Total Forecasted {target_col}: {total_forecast if isinstance(total_forecast, str) else f\"{total_forecast:.0f}\"}\n",
    "        - Average Daily {target_col}: {avg_daily_forecast if isinstance(avg_daily_forecast, str) else f\"{avg_daily_forecast:.0f}\"}\n",
    "        \n",
    "        MODEL PERFORMANCE:\n",
    "        - Mean Absolute Error (MAE): {mae if isinstance(mae, str) else f\"{mae:.2f}\"}\n",
    "        - Root Mean Squared Error (RMSE): {rmse if isinstance(rmse, str) else f\"{rmse:.2f}\"}\n",
    "        - R-squared (R²): {r2 if isinstance(r2, str) else f\"{r2:.4f}\"}\n",
    "        - Model Used: {best_model_name}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add daily forecasts if available\n",
    "        if not overall_forecast_data.empty and 'TimePeriod' in overall_forecast_data.columns:\n",
    "            report += \"\\nDAY-BY-DAY FORECAST:\\n\"\n",
    "            for i, (_, row) in enumerate(overall_forecast_data.iterrows(), 1):\n",
    "                try:\n",
    "                    forecast_date = row['TimePeriod'].strftime('%Y-%m-%d')\n",
    "                    forecast_value = row[f'{target_col}_Forecast']\n",
    "                    report += f\"    Day {i} ({forecast_date}): {forecast_value:.0f}\\n\"\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # Add product-specific forecasts if available\n",
    "        if product_forecasts:\n",
    "            report += \"\"\"\n",
    "        TOP PRODUCT FORECASTS:\n",
    "        \"\"\"\n",
    "            \n",
    "            # Select top products by forecast volume\n",
    "            top_products = []\n",
    "            for product_code, forecast_data in product_forecasts.items():\n",
    "                if 'forecasts' in forecast_data and f'{target_col}_Forecast' in forecast_data['forecasts'].columns:\n",
    "                    total_product_forecast = forecast_data['forecasts'][f'{target_col}_Forecast'].sum()\n",
    "                    description = forecast_data.get('description', f\"Product {product_code}\")\n",
    "                    top_products.append((product_code, description, total_product_forecast))\n",
    "            \n",
    "            if top_products:\n",
    "                top_products.sort(key=lambda x: x[2], reverse=True)\n",
    "                top_products = top_products[:3]  # Take top 3\n",
    "                \n",
    "                for i, (product_code, description, total_product_forecast) in enumerate(top_products, 1):\n",
    "                    forecast_data = product_forecasts[product_code]\n",
    "                    metrics = forecast_data.get('metrics', {})\n",
    "                    \n",
    "                    report += f\"\"\"\n",
    "        {i}. {description[:50]}... (Code: {product_code})\n",
    "           - Total Forecasted {target_col}: {total_product_forecast:.0f}\n",
    "           - Forecast Accuracy (R²): {metrics.get('r2', 'N/A') if isinstance(metrics.get('r2', 'N/A'), str) else f\"{metrics.get('r2', 0):.4f}\"}\n",
    "           - Mean Absolute Error: {metrics.get('mae', 'N/A') if isinstance(metrics.get('mae', 'N/A'), str) else f\"{metrics.get('mae', 0):.2f}\"}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add insights and recommendations\n",
    "        report += \"\"\"\n",
    "        KEY INSIGHTS:\n",
    "        \n",
    "        1. Demand Patterns: Analysis shows that day of week and seasonality are\n",
    "           significant factors in demand forecasting, with weekday/weekend and\n",
    "           monthly patterns clearly visible.\n",
    "        \n",
    "        2. Forecast Confidence: The overall model demonstrates good predictive\n",
    "           performance with reasonable error metrics, indicating reliable forecasts\n",
    "           for inventory planning.\n",
    "        \n",
    "        3. Product Variation: Individual product forecasts show varying levels of\n",
    "           predictability, with some products having more consistent demand patterns\n",
    "           than others.\n",
    "        \n",
    "        RECOMMENDATIONS:\n",
    "        \n",
    "        1. Inventory Planning: Adjust stock levels based on the day-by-day forecast,\n",
    "           particularly for top products with high forecast volumes.\n",
    "        \n",
    "        2. Promotional Planning: Consider scheduling promotions during forecasted\n",
    "           lower-demand periods to stimulate sales and balance demand.\n",
    "        \n",
    "        3. Product Focus: Prioritize inventory and marketing resources for the top\n",
    "           forecasted products to maximize sales potential.\n",
    "        \n",
    "        4. Model Refinement: For products with lower prediction accuracy, consider\n",
    "           collecting additional data or exploring more specialized forecasting models.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Print report\n",
    "        print(report)\n",
    "        \n",
    "        # Save report to file\n",
    "        with open(f'figure/forecast_{target_col.lower()}_report.txt', 'w') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"Forecast report saved as 'figure/forecast_{target_col.lower()}_report.txt'\")\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Provide a simplified report in case of errors\n",
    "        error_report = f\"\"\"\n",
    "        DEMAND FORECAST SUMMARY REPORT\n",
    "        ==============================\n",
    "        \n",
    "        FORECAST PERIOD: Next {horizon} days\n",
    "        TARGET METRIC: {target_col}\n",
    "        \n",
    "        NOTE: Some errors occurred during report generation: {str(e)}\n",
    "        \n",
    "        RECOMMENDATIONS:\n",
    "        \n",
    "        1. Inventory Planning: Use the generated forecast visualizations to guide inventory decisions.\n",
    "        \n",
    "        2. Model Refinement: Consider collecting additional data or exploring different forecasting models.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(error_report)\n",
    "        \n",
    "        # Save error report to file\n",
    "        with open(f'figure/forecast_{target_col.lower()}_report.txt', 'w') as f:\n",
    "            f.write(error_report)\n",
    "        \n",
    "        print(f\"Error report saved as 'figure/forecast_{target_col.lower()}_report.txt'\")\n",
    "        \n",
    "        return error_report\n",
    "\n",
    "def run_demand_forecast_analysis(file_path, target_col='Quantity', freq='D', horizon=7):\n",
    "    \"\"\"\n",
    "    Run the complete demand forecasting analysis workflow.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"MACHINE LEARNING DEMAND FORECASTING ({target_col})\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create figure directory if it doesn't exist\n",
    "    if not os.path.exists('figure'):\n",
    "        os.makedirs('figure')\n",
    "    \n",
    "    # Load and prepare data\n",
    "    df = load_and_prepare_data(file_path)\n",
    "    \n",
    "    # Prepare time series data\n",
    "    time_series_data = prepare_time_series_data(df, freq=freq)\n",
    "    \n",
    "    # # Visualize time series\n",
    "    # visualize_time_series(time_series_data)\n",
    "    \n",
    "    # Train overall demand forecast model\n",
    "    overall_forecast_results = train_overall_demand_forecast_model(time_series_data, target_col=target_col, horizon=horizon)\n",
    "    \n",
    "    # Add historical data for visualization\n",
    "    overall_forecast_results['historical_data'] = time_series_data['overall_demand'].sort_values('TimePeriod').tail(30)\n",
    "    \n",
    "    # Train product-specific forecast models\n",
    "    product_forecast_results = {}\n",
    "    for product_code in time_series_data['top_products']:\n",
    "        product_results = train_product_demand_forecast_model(time_series_data, product_code, target_col=target_col, horizon=horizon)\n",
    "        if product_results:  # Only include if model was successfully trained\n",
    "            product_forecast_results[product_code] = product_results\n",
    "    \n",
    "    # Create combined visualization\n",
    "    create_combined_forecast_visualization(overall_forecast_results, product_forecast_results, target_col=target_col)\n",
    "    \n",
    "    # Create forecast summary report\n",
    "    summary_report = create_forecast_summary_report(overall_forecast_results, product_forecast_results, target_col=target_col, horizon=horizon)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Return forecasting results\n",
    "    return {\n",
    "        'overall_forecast': overall_forecast_results,\n",
    "        'product_forecasts': product_forecast_results,\n",
    "        'time_series_data': time_series_data,\n",
    "        'summary_report': summary_report\n",
    "    }\n",
    "\n",
    "# If running as a script\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the analysis for both quantity and revenue\n",
    "    file_path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx\"  # Update with your file path\n",
    "    \n",
    "    # Forecast revenue\n",
    "    revenue_results = run_demand_forecast_analysis(file_path, target_col='Revenue', horizon=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Demand Forecasting - XGBoost Regression (machine-learning-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running as a script\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the analysis for both quantity and revenue\n",
    "    file_path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx\"  # Update with your file path\n",
    "    \n",
    "    # Forecast quantity\n",
    "    quantity_results = run_demand_forecast_analysis(file_path, target_col='Quantity', horizon=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Price Optimization - K-means Clustering & Linear Regrassion (hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class RetailPriceOptimizer:\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"Initialize with the file path to the retail dataset.\"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.data = None\n",
    "        self.product_metrics = None\n",
    "        self.elasticity_results = None\n",
    "        self.price_recommendations = None\n",
    "    \n",
    "    def load_and_preprocess_data(self):\n",
    "        \"\"\"Load and preprocess the retail data.\"\"\"\n",
    "        print(\"Loading and preprocessing data...\")\n",
    "        \n",
    "        # Load the data\n",
    "        self.data = pd.read_excel(self.file_path)\n",
    "        \n",
    "        # Basic data cleaning\n",
    "        # Remove rows with missing values in essential columns\n",
    "        self.data = self.data.dropna(subset=['InvoiceNo', 'StockCode', 'Quantity', 'UnitPrice'])\n",
    "        \n",
    "        # Convert date column to datetime\n",
    "        self.data['InvoiceDate'] = pd.to_datetime(self.data['InvoiceDate'])\n",
    "        \n",
    "        # Filter out canceled orders (invoices starting with 'C')\n",
    "        if self.data['InvoiceNo'].dtype == 'object':\n",
    "            self.data = self.data[~self.data['InvoiceNo'].astype(str).str.startswith('C')]\n",
    "        \n",
    "        # Filter out negative quantities (returns)\n",
    "        self.data_positive = self.data[self.data['Quantity'] > 0].copy()\n",
    "        \n",
    "        # Filter out extreme values\n",
    "        q_low = self.data_positive['UnitPrice'].quantile(0.01)\n",
    "        q_high = self.data_positive['UnitPrice'].quantile(0.99)\n",
    "        self.data_filtered = self.data_positive[(self.data_positive['UnitPrice'] >= q_low) & \n",
    "                                              (self.data_positive['UnitPrice'] <= q_high)]\n",
    "        \n",
    "        # Calculate revenue\n",
    "        self.data_filtered['Revenue'] = self.data_filtered['Quantity'] * self.data_filtered['UnitPrice']\n",
    "        \n",
    "        print(f\"Data loaded and preprocessed. {len(self.data_filtered)} valid transactions.\")\n",
    "        print(f\"Date range: {self.data_filtered['InvoiceDate'].min().date()} to {self.data_filtered['InvoiceDate'].max().date()}\")\n",
    "        \n",
    "        # Basic summary\n",
    "        print(\"\\nBasic Statistics:\")\n",
    "        print(f\"Total Transactions: {self.data_filtered['InvoiceNo'].nunique()}\")\n",
    "        print(f\"Total Products: {self.data_filtered['StockCode'].nunique()}\")\n",
    "        print(f\"Total Customers: {self.data_filtered['CustomerID'].dropna().nunique()}\")\n",
    "        print(f\"Price Range: ${self.data_filtered['UnitPrice'].min():.2f} to ${self.data_filtered['UnitPrice'].max():.2f}\")\n",
    "        \n",
    "        return self.data_filtered\n",
    "    \n",
    "    def calculate_product_metrics(self):\n",
    "        \"\"\"Calculate key metrics for each product.\"\"\"\n",
    "        print(\"\\nCalculating product metrics...\")\n",
    "        \n",
    "        if self.data_filtered is None:\n",
    "            raise ValueError(\"No data available. Run load_and_preprocess_data first.\")\n",
    "        \n",
    "        # Group by product\n",
    "        self.product_metrics = self.data_filtered.groupby('StockCode').agg({\n",
    "            'Description': 'first',  # Get product description\n",
    "            'Quantity': ['sum', 'mean', 'count'],  # Sales volume metrics\n",
    "            'UnitPrice': ['mean', 'min', 'max', 'std'],  # Price metrics\n",
    "            'Revenue': 'sum',  # Revenue\n",
    "            'InvoiceNo': 'nunique',  # Number of orders\n",
    "            'CustomerID': lambda x: x.dropna().nunique()  # Number of unique customers\n",
    "        })\n",
    "        \n",
    "        # Flatten the column hierarchy\n",
    "        self.product_metrics.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in self.product_metrics.columns]\n",
    "        \n",
    "        # Rename columns for clarity\n",
    "        self.product_metrics = self.product_metrics.rename(columns={\n",
    "            'Description_first': 'Description',\n",
    "            'Quantity_sum': 'TotalQuantity',\n",
    "            'Quantity_mean': 'AvgOrderSize',\n",
    "            'Quantity_count': 'TransactionCount',\n",
    "            'UnitPrice_mean': 'AvgPrice',\n",
    "            'UnitPrice_min': 'MinPrice',\n",
    "            'UnitPrice_max': 'MaxPrice',\n",
    "            'UnitPrice_std': 'PriceStdDev',\n",
    "            'Revenue_sum': 'TotalRevenue',\n",
    "            'InvoiceNo_nunique': 'OrderCount',\n",
    "            'CustomerID_<lambda>': 'CustomerCount'\n",
    "        })\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        self.product_metrics['PriceVariation'] = self.product_metrics['PriceStdDev'] / self.product_metrics['AvgPrice']\n",
    "        self.product_metrics['PriceVariation'] = self.product_metrics['PriceVariation'].fillna(0)\n",
    "        \n",
    "        # Flag products with sufficient price variation for elasticity analysis\n",
    "        self.product_metrics['HasPriceVariation'] = self.product_metrics['PriceVariation'] > 0.01  # 1% threshold\n",
    "        \n",
    "        self.product_metrics = self.product_metrics.reset_index()\n",
    "        \n",
    "        # Display summary\n",
    "        print(f\"Calculated metrics for {len(self.product_metrics)} products\")\n",
    "        price_var_count = sum(self.product_metrics['HasPriceVariation'])\n",
    "        print(f\"{price_var_count} products have price variation (helpful for elasticity calculation)\")\n",
    "        \n",
    "        return self.product_metrics\n",
    "    \n",
    "    def analyze_top_products(self, n=10):\n",
    "        \"\"\"Display the top n products by revenue.\"\"\"\n",
    "        if self.product_metrics is None:\n",
    "            self.calculate_product_metrics()\n",
    "        \n",
    "        top_products = self.product_metrics.nlargest(n, 'TotalRevenue')\n",
    "        \n",
    "        print(f\"\\nTop {n} Products by Revenue:\")\n",
    "        for i, row in top_products.iterrows():\n",
    "            print(f\"{row['StockCode']} - {row['Description']}\")\n",
    "            print(f\"  Revenue: ${row['TotalRevenue']:.2f}\")\n",
    "            print(f\"  Quantity: {row['TotalQuantity']}\")\n",
    "            print(f\"  Avg Price: ${row['AvgPrice']:.2f}\")\n",
    "            print(f\"  Price Range: ${row['MinPrice']:.2f} - ${row['MaxPrice']:.2f}\")\n",
    "            print(f\"  Price Variation: {row['PriceVariation']:.2%}\")\n",
    "            print()\n",
    "        \n",
    "        return top_products\n",
    "    \n",
    "    def calculate_price_elasticity(self, min_transactions=5, show_plots=True):\n",
    "        \"\"\"Calculate price elasticity for products with sufficient variation.\"\"\"\n",
    "        print(\"\\nCalculating price elasticity...\")\n",
    "        \n",
    "        if self.product_metrics is None:\n",
    "            self.calculate_product_metrics()\n",
    "        \n",
    "        # Get products with price variation\n",
    "        products_with_variation = self.product_metrics[\n",
    "            (self.product_metrics['HasPriceVariation']) & \n",
    "            (self.product_metrics['TransactionCount'] >= min_transactions)\n",
    "        ]['StockCode'].unique()\n",
    "        \n",
    "        print(f\"Analyzing {len(products_with_variation)} products with sufficient price variation\")\n",
    "        \n",
    "        elasticity_results = []\n",
    "        elasticity_plots = []\n",
    "        \n",
    "        for product_id in products_with_variation:\n",
    "            # Get data for this product\n",
    "            product_data = self.data_filtered[self.data_filtered['StockCode'] == product_id].copy()\n",
    "            \n",
    "            # Group data by price point to get quantity at each price\n",
    "            price_qty = product_data.groupby('UnitPrice').agg({\n",
    "                'Quantity': 'sum',\n",
    "                'InvoiceNo': 'nunique'\n",
    "            }).reset_index()\n",
    "            \n",
    "            # Skip if not enough distinct price points\n",
    "            if len(price_qty) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Calculate average quantity per order at each price\n",
    "            price_qty['AvgQuantityPerOrder'] = price_qty['Quantity'] / price_qty['InvoiceNo']\n",
    "            \n",
    "            # Log transformation for elasticity calculation\n",
    "            X = np.log(price_qty['UnitPrice']).values.reshape(-1, 1)\n",
    "            y = np.log(price_qty['AvgQuantityPerOrder']).values\n",
    "            \n",
    "            try:\n",
    "                # Linear regression on log-transformed data\n",
    "                model = LinearRegression()\n",
    "                model.fit(X, y)\n",
    "                \n",
    "                # Price elasticity is the coefficient in log-log model\n",
    "                elasticity = model.coef_[0]\n",
    "                \n",
    "                # R-squared value to measure model fit\n",
    "                r2 = model.score(X, y)\n",
    "                \n",
    "                # Only include results with reasonable elasticity estimates and good fit\n",
    "                if -10 < elasticity < 10 and r2 > 0.3:\n",
    "                    # Get product details\n",
    "                    product_info = self.product_metrics[self.product_metrics['StockCode'] == product_id].iloc[0]\n",
    "                    avg_price = product_info['AvgPrice']\n",
    "                    total_revenue = product_info['TotalRevenue']\n",
    "                    \n",
    "                    # Determine optimal price direction based on elasticity\n",
    "                    if elasticity < -1:  # Elastic demand\n",
    "                        optimal_price_factor = 0.9  # Reduce price for elastic goods\n",
    "                        price_action = \"Decrease\"\n",
    "                    elif -1 < elasticity < 0:  # Inelastic demand\n",
    "                        optimal_price_factor = 1.1  # Increase price for inelastic goods\n",
    "                        price_action = \"Increase\"\n",
    "                    else:\n",
    "                        optimal_price_factor = 1.0  # Maintain price\n",
    "                        price_action = \"Maintain\"\n",
    "                    \n",
    "                    optimal_price = avg_price * optimal_price_factor\n",
    "                    \n",
    "                    # Calculate potential revenue with optimal price\n",
    "                    # Using constant elasticity model: Q2 = Q1 * (P2/P1)^elasticity\n",
    "                    current_quantity = product_info['TotalQuantity']\n",
    "                    estimated_quantity = current_quantity * (optimal_price / avg_price) ** elasticity\n",
    "                    potential_revenue = optimal_price * estimated_quantity\n",
    "                    revenue_impact = potential_revenue - total_revenue\n",
    "                    \n",
    "                    elasticity_results.append({\n",
    "                        'StockCode': product_id,\n",
    "                        'Description': product_info['Description'],\n",
    "                        'Elasticity': elasticity,\n",
    "                        'R2': r2,\n",
    "                        'AvgPrice': avg_price,\n",
    "                        'OptimalPrice': optimal_price,\n",
    "                        'PriceAction': price_action,\n",
    "                        'PriceChange': (optimal_price - avg_price) / avg_price * 100,\n",
    "                        'CurrentRevenue': total_revenue,\n",
    "                        'PotentialRevenue': potential_revenue,\n",
    "                        'RevenueImpact': revenue_impact,\n",
    "                        'RevenueChangePercent': revenue_impact / total_revenue * 100\n",
    "                    })\n",
    "                    \n",
    "                    # Store data for plotting\n",
    "                    if show_plots and len(elasticity_results) <= 5:  # Limit to first 5 for plots\n",
    "                        elasticity_plots.append({\n",
    "                            'product_id': product_id,\n",
    "                            'description': product_info['Description'],\n",
    "                            'price_qty': price_qty,\n",
    "                            'elasticity': elasticity,\n",
    "                            'model': model\n",
    "                        })\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating elasticity for product {product_id}: {e}\")\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        if elasticity_results:\n",
    "            self.elasticity_results = pd.DataFrame(elasticity_results)\n",
    "            \n",
    "            # Display summary\n",
    "            print(f\"Successfully calculated elasticity for {len(self.elasticity_results)} products\")\n",
    "            print(\"\\nElasticity Summary:\")\n",
    "            print(f\"Mean elasticity: {self.elasticity_results['Elasticity'].mean():.2f}\")\n",
    "            print(f\"Elastic products (e < -1): {sum(self.elasticity_results['Elasticity'] < -1)}\")\n",
    "            print(f\"Inelastic products (-1 < e < 0): {sum((self.elasticity_results['Elasticity'] > -1) & (self.elasticity_results['Elasticity'] < 0))}\")\n",
    "            \n",
    "            # Display example elasticity values\n",
    "            print(\"\\nSample Elasticity Results:\")\n",
    "            sample = self.elasticity_results.head(5)\n",
    "            for _, row in sample.iterrows():\n",
    "                print(f\"{row['StockCode']} - {row['Description']}\")\n",
    "                print(f\"  Elasticity: {row['Elasticity']:.2f} (R² = {row['R2']:.2f})\")\n",
    "                print(f\"  Current Price: ${row['AvgPrice']:.2f}\")\n",
    "                print(f\"  Recommendation: {row['PriceAction']} price to ${row['OptimalPrice']:.2f} ({row['PriceChange']:+.1f}%)\")\n",
    "                print(f\"  Revenue Impact: ${row['RevenueImpact']:.2f} ({row['RevenueChangePercent']:+.1f}%)\")\n",
    "                print()\n",
    "            \n",
    "            # Generate plots for sample products\n",
    "            if show_plots and elasticity_plots:\n",
    "                plt.figure(figsize=(15, 10))\n",
    "                \n",
    "                for i, plot_data in enumerate(elasticity_plots):\n",
    "                    plt.subplot(2, 3, i+1)\n",
    "                    \n",
    "                    # Original data points\n",
    "                    plt.scatter(plot_data['price_qty']['UnitPrice'], \n",
    "                              plot_data['price_qty']['AvgQuantityPerOrder'],\n",
    "                              color='blue', alpha=0.7)\n",
    "                    \n",
    "                    # Generate prediction line\n",
    "                    price_range = np.linspace(\n",
    "                        plot_data['price_qty']['UnitPrice'].min() * 0.9,\n",
    "                        plot_data['price_qty']['UnitPrice'].max() * 1.1,\n",
    "                        20\n",
    "                    )\n",
    "                    log_qty_pred = plot_data['model'].predict(np.log(price_range).reshape(-1, 1))\n",
    "                    qty_pred = np.exp(log_qty_pred)\n",
    "                    \n",
    "                    plt.plot(price_range, qty_pred, 'r-')\n",
    "                    \n",
    "                    plt.title(f\"{plot_data['product_id']}: e={plot_data['elasticity']:.2f}\")\n",
    "                    plt.xlabel('Price')\n",
    "                    plt.ylabel('Quantity')\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        \n",
    "        else:\n",
    "            print(\"No products with sufficient data for elasticity calculation\")\n",
    "            self.elasticity_results = pd.DataFrame()\n",
    "        \n",
    "        return self.elasticity_results\n",
    "    \n",
    "    def generate_price_recommendations(self):\n",
    "        \"\"\"Generate final price recommendations based on elasticity analysis.\"\"\"\n",
    "        print(\"\\nGenerating price recommendations...\")\n",
    "        \n",
    "        if self.elasticity_results is None or self.elasticity_results.empty:\n",
    "            self.calculate_price_elasticity(show_plots=False)\n",
    "        \n",
    "        if self.elasticity_results.empty:\n",
    "            print(\"Insufficient data for price recommendations.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Create recommendations\n",
    "        self.price_recommendations = self.elasticity_results.copy()\n",
    "        \n",
    "        # Add recommendation strength based on revenue impact and statistical confidence\n",
    "        conditions = [\n",
    "            (self.price_recommendations['R2'] > 0.7) & (abs(self.price_recommendations['RevenueChangePercent']) > 5),\n",
    "            (self.price_recommendations['R2'] > 0.5) & (abs(self.price_recommendations['RevenueChangePercent']) > 2),\n",
    "            (self.price_recommendations['R2'] > 0.3) & (abs(self.price_recommendations['RevenueChangePercent']) > 1)\n",
    "        ]\n",
    "        choices = ['Strong', 'Moderate', 'Weak']\n",
    "        self.price_recommendations['RecommendationStrength'] = np.select(conditions, choices, default='Very Weak')\n",
    "        \n",
    "        # Sort by potential revenue impact\n",
    "        self.price_recommendations = self.price_recommendations.sort_values('RevenueImpact', ascending=False)\n",
    "        \n",
    "        # Format for display\n",
    "        self.price_recommendations['FormattedRecommendation'] = self.price_recommendations.apply(\n",
    "            lambda x: (\n",
    "                f\"{x['PriceAction']} price from ${x['AvgPrice']:.2f} to ${x['OptimalPrice']:.2f} \"\n",
    "                f\"({'+' if x['PriceChange'] > 0 else ''}{x['PriceChange']:.1f}%). \"\n",
    "                f\"Estimated revenue impact: ${x['RevenueImpact']:.2f} \"\n",
    "                f\"({'+' if x['RevenueChangePercent'] > 0 else ''}{x['RevenueChangePercent']:.1f}%). \"\n",
    "                f\"{x['RecommendationStrength']} recommendation.\"\n",
    "            ), axis=1\n",
    "        )\n",
    "        \n",
    "        # Display top recommendations\n",
    "        print(\"\\nTop Price Optimization Recommendations:\")\n",
    "        top_recs = self.price_recommendations.head(10)\n",
    "        for _, row in top_recs.iterrows():\n",
    "            print(f\"{row['StockCode']} - {row['Description']}\")\n",
    "            print(f\"  {row['FormattedRecommendation']}\")\n",
    "            print()\n",
    "        \n",
    "        # Visualize recommendations\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Select top products with highest absolute revenue impact\n",
    "        plot_data = self.price_recommendations.nlargest(10, 'RevenueImpact')\n",
    "        \n",
    "        # Create bar chart\n",
    "        bars = plt.bar(range(len(plot_data)), plot_data['RevenueImpact'], color='g')\n",
    "        \n",
    "        # Color negative impacts in red\n",
    "        for i, impact in enumerate(plot_data['RevenueImpact']):\n",
    "            if impact < 0:\n",
    "                bars[i].set_color('r')\n",
    "        \n",
    "        # Add labels\n",
    "        plt.xticks(range(len(plot_data)), plot_data['StockCode'], rotation=45, ha='right')\n",
    "        plt.xlabel('Product')\n",
    "        plt.ylabel('Revenue Impact ($)')\n",
    "        plt.title('Top 10 Products by Revenue Optimization Potential')\n",
    "        \n",
    "        # Add price change annotations\n",
    "        for i, row in enumerate(plot_data.iterrows()):\n",
    "            row = row[1]  # Get the actual row data\n",
    "            plt.text(\n",
    "                i, row['RevenueImpact'], \n",
    "                f\"{row['PriceChange']:+.1f}%\", \n",
    "                ha='center', \n",
    "                va='bottom' if row['RevenueImpact'] > 0 else 'top'\n",
    "            )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return self.price_recommendations\n",
    "    \n",
    "    def cluster_products_by_price_sensitivity(self, n_clusters=4):\n",
    "        \"\"\"Cluster products based on price sensitivity and other metrics.\"\"\"\n",
    "        print(\"\\nClustering products by price sensitivity...\")\n",
    "        \n",
    "        if self.product_metrics is None:\n",
    "            self.calculate_product_metrics()\n",
    "        \n",
    "        if self.elasticity_results is None or self.elasticity_results.empty:\n",
    "            self.calculate_price_elasticity(show_plots=False)\n",
    "        \n",
    "        # Prepare data for clustering\n",
    "        # Merge product metrics with elasticity if available\n",
    "        if not self.elasticity_results.empty:\n",
    "            clustering_data = self.product_metrics.merge(\n",
    "                self.elasticity_results[['StockCode', 'Elasticity']], \n",
    "                on='StockCode', \n",
    "                how='left'\n",
    "            )\n",
    "        else:\n",
    "            clustering_data = self.product_metrics.copy()\n",
    "            clustering_data['Elasticity'] = np.nan\n",
    "        \n",
    "        # Select features for clustering\n",
    "        features = ['AvgPrice', 'TotalQuantity', 'PriceVariation', 'TotalRevenue']\n",
    "        \n",
    "        # Add elasticity if available\n",
    "        if 'Elasticity' in clustering_data.columns:\n",
    "            # Fill missing elasticities with mean\n",
    "            mean_elasticity = clustering_data['Elasticity'].mean()\n",
    "            if not np.isnan(mean_elasticity):\n",
    "                clustering_data['Elasticity'] = clustering_data['Elasticity'].fillna(mean_elasticity)\n",
    "                features.append('Elasticity')\n",
    "        \n",
    "        # Prepare data\n",
    "        X = clustering_data[features].copy()\n",
    "        \n",
    "        # Handle missing values\n",
    "        X = X.fillna(X.mean())\n",
    "        \n",
    "        # Scale the data\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Apply KMeans clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        clustering_data['Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "        \n",
    "        # Calculate cluster statistics\n",
    "        cluster_stats = clustering_data.groupby('Cluster').agg({\n",
    "            'StockCode': 'count',\n",
    "            'AvgPrice': 'mean',\n",
    "            'TotalQuantity': 'mean',\n",
    "            'TotalRevenue': 'mean',\n",
    "            'PriceVariation': 'mean'\n",
    "        }).rename(columns={'StockCode': 'ProductCount'})\n",
    "        \n",
    "        # Add elasticity stats if available\n",
    "        if 'Elasticity' in clustering_data.columns:\n",
    "            elasticity_stats = clustering_data.groupby('Cluster')['Elasticity'].mean()\n",
    "            cluster_stats = cluster_stats.join(elasticity_stats)\n",
    "        \n",
    "        print(\"\\nProduct Clusters by Price Sensitivity:\")\n",
    "        print(cluster_stats)\n",
    "        \n",
    "        # Visualize clusters\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot avg price vs total revenue colored by cluster\n",
    "        plt.scatter(\n",
    "            clustering_data['AvgPrice'], \n",
    "            clustering_data['TotalRevenue'],\n",
    "            c=clustering_data['Cluster'], \n",
    "            cmap='viridis', \n",
    "            alpha=0.6,\n",
    "            s=100  # Point size\n",
    "        )\n",
    "        \n",
    "        plt.xlabel('Average Price ($)')\n",
    "        plt.ylabel('Total Revenue ($)')\n",
    "        plt.title('Product Clusters by Price and Revenue')\n",
    "        \n",
    "        # Add annotation for selected products\n",
    "        top_per_cluster = clustering_data.loc[clustering_data.groupby('Cluster')['TotalRevenue'].idxmax()]\n",
    "        \n",
    "        for _, row in top_per_cluster.iterrows():\n",
    "            plt.annotate(\n",
    "                row['StockCode'],\n",
    "                (row['AvgPrice'], row['TotalRevenue']),\n",
    "                xytext=(5, 5),\n",
    "                textcoords='offset points'\n",
    "            )\n",
    "        \n",
    "        plt.colorbar(label='Cluster')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return clustering_data\n",
    "    \n",
    "    def run_optimization_pipeline(self):\n",
    "        \"\"\"Run the full price optimization pipeline.\"\"\"\n",
    "        # 1. Load and preprocess data\n",
    "        self.load_and_preprocess_data()\n",
    "        \n",
    "        # 2. Calculate product metrics\n",
    "        self.calculate_product_metrics()\n",
    "        \n",
    "        # 3. Analyze top products\n",
    "        self.analyze_top_products()\n",
    "        \n",
    "        # 4. Calculate price elasticity\n",
    "        self.calculate_price_elasticity()\n",
    "        \n",
    "        # 5. Generate price recommendations\n",
    "        self.generate_price_recommendations()\n",
    "        \n",
    "        # 6. Cluster products\n",
    "        self.cluster_products_by_price_sensitivity()\n",
    "        \n",
    "        print(\"\\nPrice optimization analysis complete!\")\n",
    "        \n",
    "        return self.price_recommendations\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    optimizer = RetailPriceOptimizer(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx\")\n",
    "    recommendations = optimizer.run_optimization_pipeline()\n",
    "    \n",
    "    # Export recommendations to CSV if needed\n",
    "    if not recommendations.empty:\n",
    "        recommendations.to_csv(\"data/price_recommendations.csv\", index=False)\n",
    "        print(\"Recommendations exported to price_recommendations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Wrap-up\n",
    "<blockquote style=\"font-size: 30px; margin-left: 20px;\">\n",
    "\"Numbers have an important story to tell. They rely on you to give them a clear and convincing voice.\"\n",
    "</blockquote>\n",
    "\n",
    "![Compact Graph - Bad Alternative](https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/compact_graphs.png)\n",
    "[Image credit: xkcd](https://xkcd.com/2864)\n",
    "\n",
    "\n",
    "## Future Learning Resources\n",
    "\n",
    "- [Deep Dive into LLMs like ChatGPT](https://youtu.be/7xTGNNLPyMI?si=tuOTm8Zdv8GycV6R) - a general audience deep dive into the Large Language Model (LLM) AI technology that powers ChatGPT and related products. It is covers the full training stack of how the models are developed, along with mental models of how to think about their \"psychology\", and how to get the best use them in practical applications.\n",
    "- [Prompt Engineering](https://www.promptingguide.ai) - a comprehensive resource designed to help users master the art of prompt engineering for AI models. It provides structured guidance, best practices, and practical examples to improve interactions with AI systems like ChatGPT, Claude, and other LLMs.\n",
    "- [Hugging Face](https://huggingface.co/)- a leading platform for open-source AI, specializing in natural language processing (NLP), machine learning models, and AI collaboration tools. It provides a comprehensive ecosystem for developers, researchers, and organizations to build, train, and deploy AI models efficiently.\n",
    "- [Mckinsey Report1 - AI in Company](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/superagency-in-the-workplace-empowering-people-to-unlock-ais-full-potential-at-work) (https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Post-measurements\n",
    "\n",
    "## Post-assessments (post knowledge)\n",
    "* [Link](https://ncsu.qualtrics.com/jfe/form/SV_8qfQ1kyxCpcRMrQ)\n",
    "* <img src=\"https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/post_assess.png\" alt=\"QR\" width=\"200\">\n",
    "\n",
    "## Post-survey (post experience)\n",
    "* [Link](https://ncsu.qualtrics.com/jfe/form/SV_6VTIyENQC8rew98)\n",
    "* <img src=\"https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/post_survey.png\" alt=\"QR\" width=\"200\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Python_Open_Labs_Week1_filled.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
