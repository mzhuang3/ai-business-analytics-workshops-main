{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Welcome and Introducation\n",
    "\n",
    "## Who we are\n",
    "![Team](https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/team.png)\n",
    "\n",
    "## Our collaborative objectives\n",
    "> Implement a series of AI-integrated workshops on data-driven decision making, aiming to enrich prospective students' learning experiences and encourage increased participation and success in the business analytics program. \n",
    " \n",
    "\n",
    "---\n",
    "\n",
    "## Workshop agenda\n",
    "0. [Setup](#Setup)\n",
    "1. [Welcome and Introduction](#1-welcome-and-introducation)\n",
    "2. [Consent and Pre-measurements](#2-consent-and-pre-measurements)\n",
    "3. [Lecture](#3-lecture)\n",
    "4. [Demo](#4-demo)\n",
    "5. [Wrap-up](#5-wrap-up)\n",
    "6. [Post-measurements](#6-post-measurements) \n",
    "\n",
    "---\n",
    "\n",
    "## Workshop series\n",
    "* Workshop 1:\n",
    "    * Understand fundamental AI concepts\n",
    "    * Identify specific AI use cases in business scenarios, with a focus on AI in data modeling\n",
    "    * Experiment with AI tool (i.e., LLMs-based agents) deployment as an assistant for business analysts\n",
    "        * Analytics task: Understanding your customer\n",
    "            * customer segementation\n",
    "            * co-buying behaviors\n",
    "    * Recognize AI's dual role in enhancing business analytics - both in data modeling and as an assitant for business analysts\n",
    "\n",
    "<div style=\"color: lightgrey;\">\n",
    "* Workshop 2:\n",
    "    * Understand the role of data visualization in business analytics \n",
    "    * Identify basic chart types and when to use\n",
    "    * Experiment with AI tool (i.e., LLMs-based agents) deployment as an assistant for business analysts\n",
    "        * Analytics task: Understanding your product\n",
    "            * historical performance\n",
    "            * revenue prediction\n",
    "            * demand forecasting\n",
    "            * price optimization\n",
    "</div>\n",
    "\n",
    "<div style=\"color: lightgrey;\">\n",
    "* Workshop 3:\n",
    "    * Understand general AI ethical challenges\n",
    "    * Identify AI ethics in business analytics\n",
    "    * Gain insights from a business analyst professional\n",
    "        * Explore the career paths\n",
    "        * Identify essential skills\n",
    "        * Understand AI’s impact on the industry\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Consent and Pre-measurements\n",
    "\n",
    "## Consent Form\n",
    "- [Link](https://docs.google.com/forms/d/e/1FAIpQLSc3DNPf8Yu99O4crWiQMdGzJli8R4QlmlMsL04aYDG2Y2vMvw/viewform?usp=share_link)\n",
    "- <img src=\"https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/consent.png\" alt=\"QR\" width=\"200\">\n",
    "\n",
    "## Pre-assessments (prior knowledge)\n",
    "- [Link](https://ncsu.qualtrics.com/jfe/form/SV_6Wf0Ix7EdB9NDpA)\n",
    "- <img src=\"https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/pre_assess.png\" alt=\"QR\" width=\"200\">\n",
    "\n",
    "\n",
    "## Pre-survey (prior experience)\n",
    "* [Link](https://ncsu.qualtrics.com/jfe/form/SV_3JdVSGjdZP7qjSm)\n",
    "* <img src=\"https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/pre_survey.png\" alt=\"QR\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Lecture\n",
    "\n",
    "## Today's Topics\n",
    "- What are the AI-relevant and technical concepts?\n",
    "- What is AI life cycle?\n",
    "- What is Business Analytics cycle?\n",
    "- What are the AI use cases in Business Analytics?\n",
    "\n",
    "\n",
    "## AI Basics\n",
    "### (1). AI-relevant Concepts\n",
    "![AI-relevant Concepts](https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/ai_concepts.png)\n",
    "[Image credit: Sarker (2024)](https://doi.org/10.1007/978-3-031-54497-2_5)\n",
    "\n",
    "### (2). AI-technical Concepts\n",
    "![AI-technical Concepts](https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/machine_learning.png)\n",
    "[Image credit: Oniani et al. (2023)](https://doi.org/10.1038/s41746-023-00965-x)\n",
    "\n",
    "![Supervised Learning Diagram](https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/supervised.png)\n",
    "[Image credit: Abeyon](https://abeyon.com/how-do-machines-learn/)\n",
    "![Unsupervised Learning Diagram](https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/unsupervised.png)\n",
    "[Image credit: Abeyon](https://abeyon.com/how-do-machines-learn/)\n",
    "\n",
    "![Reinforcement Learning Diagram](https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/reinforcement.png)\n",
    "[Image credit: MathWorks](https://www.mathworks.com/discovery/reinforcement-learning.html)\n",
    "\n",
    "### (3). AI Life Cycle\n",
    "![AI Life Cycle](https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/ai_life_cycle.png)\n",
    "[Image credit: Ng et al. (2022)](https://doi.org/10.1038/s41591-022-01993-y)\n",
    "\n",
    "\n",
    "## Business Analytics\n",
    "### Business Analytic Cycle\n",
    "![Business Analytics Cycle](https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/ba_cycle.png)\n",
    "[Image credit: Michigan State University](https://www.michiganstateuniversityonline.com/resources/business-analytics/business-solutions-with-business-analytics-cycle/)\n",
    "\n",
    "\n",
    "## AI Use Cases in Business Analytics\n",
    "### (1). Customer Segementation\n",
    "![Customer Segementation](https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/customer_segementation.png)\n",
    "[Image credit: Medium](https://medium.com/data-science/how-to-automatically-segment-customers-using-purchase-data-and-a-few-lines-of-python-36939fb587a4)\n",
    "\n",
    "### (2). Demand Forecasting\n",
    "![Demand Forecasting](https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/demand_forecasting.png)\n",
    "[Image credit: Byvi](https://byvi.co/2023/04/26/the-role-of-ai-and-machine-learning-in-demand-forecasting/)\n",
    "\n",
    "### (3). Frad Detection\n",
    "![Frad Detection](https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/fraud_detection.png)\n",
    "[Image credit: Ragate team](https://www.researchgate.net/publication/321682378_Pattern_Recognition_Anomaly_Detection_Challenges)\n",
    "\n",
    "### (4). Cooccurence Events\n",
    "![Customer Cobuying Behaviors](https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/customer_cobuying.png)\n",
    "[Image credit: Market Basket Learning](https://marketbasketlearning.wordpress.com/decision-trees/market-basket-analysis/)\n",
    "\n",
    "### (5). Conversational Agents\n",
    "![Conversational Agents](https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/conversational_agent.png)\n",
    "[Image credit: Akira](https://www.akira.ai/blog/customer-service-with-advanced-agentic-ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Demo\n",
    "## (1). Dataset\n",
    "\n",
    "In this section, we will explore how to deploy AI tools (i.e., LLMs-based agents) to conduct both traditional and machine-learning-based business analytics using data from the [Online Retail dataset](https://archive.ics.uci.edu/dataset/352/online+retail). \n",
    "\n",
    "> The Online Retail dataset, available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu), contains transactional data from a UK-based online retail store. It is commonly used for data analysis, machine learning, and business intelligence applications such as customer segmentation, sales forecasting, and market basket analysis.\n",
    "\n",
    "#### Dataset Overview\n",
    "* Source: Transactions from an anonymous UK-based e-commerce store.\n",
    "* Time Period: One year (from December 2010 to December 2011).\n",
    "* Number of Records: 541,909 transactions.\n",
    "* Number of Features: 8 columns describing each transaction.\n",
    "\n",
    "#### Features in the Dataset\n",
    "* InvoiceNo – Unique identifier for each transaction.\n",
    "* StockCode – Product code associated with each item.\n",
    "* Description – Name or description of the product.\n",
    "* Quantity – Number of items purchased in the transaction.\n",
    "* InvoiceDate – Date and time of the transaction.\n",
    "* UnitPrice – Price of a single unit of the product (in GBP).\n",
    "* CustomerID – Unique identifier for customers (missing for some transactions).\n",
    "* Country – Country of the customer making the purchase.\n",
    "\n",
    "## (2). Your Tasks as a Business Analyst\n",
    "> From your manager <br>\n",
    "> \"I'd like you to lead an initiative to deepen our understanding of our customer base to inform our strategic decisions in the coming quarters\"\n",
    "\n",
    "## (3). Use AI Tools (i.e., LLMs) as Your Assistant\n",
    "[Sample chat history with Claude](https://claude.ai/share/72a38f9e-db77-46dc-907e-f781e9289bcc)\n",
    "\n",
    "## (4). Experiment on AI-assisted Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Segmentation\n",
    "### a. RFM Analysis (non-machine-learning)\n",
    "\n",
    "#### Key Components of RFM Analysis\n",
    "* Recency (R) – How recently a customer made a purchase.\n",
    "    Customers who purchased recently are more likely to engage again.\n",
    "* Frequency (F) – How often a customer purchases.\n",
    "    Frequent buyers are usually more loyal.\n",
    "* Monetary (M) – How much a customer spends in total.\n",
    "    High spenders contribute more to revenue.\n",
    "\n",
    "#### How It Works\n",
    "1. Assign scores (e.g., 1–5) to each customer for Recency, Frequency, and Monetary based on their ranking within the dataset.\n",
    "2. Combine these scores to classify customers into groups (e.g., VIPs, at-risk customers, inactive customers).\n",
    "3. Use these insights for targeted marketing, customer retention, and loyalty programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set plotting style - updated for newer versions of matplotlib/seaborn\n",
    "plt.style.use('default')  # Use default style instead of deprecated 'seaborn-whitegrid'\n",
    "sns.set_theme()  # Modern way to set seaborn defaults\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "# Load the data\n",
    "def load_and_prepare_data(file_path):\n",
    "    \"\"\"\n",
    "    Load Excel file and prepare data for RFM analysis.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    print(\"\\nDataset Overview:\")\n",
    "    print(f\"- Shape: {df.shape}\")\n",
    "    print(f\"- Columns: {', '.join(df.columns)}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"\\nMissing Values:\")\n",
    "    for col, count in missing_values.items():\n",
    "        if count > 0:\n",
    "            print(f\"- {col}: {count} missing values ({count/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Filter out records with missing CustomerID\n",
    "    df_clean = df.dropna(subset=['CustomerID'])\n",
    "    df_clean['CustomerID'] = df_clean['CustomerID'].astype(int)\n",
    "    \n",
    "    # Check for negative quantities (returns)\n",
    "    returns = df_clean[df_clean['Quantity'] < 0]\n",
    "    print(f\"\\nReturns identified: {len(returns)} records\")\n",
    "    \n",
    "    # Calculate total value of each transaction\n",
    "    df_clean['TotalValue'] = df_clean['Quantity'] * df_clean['UnitPrice']\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Perform RFM analysis\n",
    "def perform_rfm_analysis(df, analysis_date=None):\n",
    "    \"\"\"\n",
    "    Calculate RFM metrics for each customer.\n",
    "    \"\"\"\n",
    "    # If no analysis date provided, use the max date in the dataset + 1 day\n",
    "    if analysis_date is None:\n",
    "        analysis_date = df['InvoiceDate'].max() + timedelta(days=1)\n",
    "        print(f\"Using analysis date: {analysis_date}\")\n",
    "    \n",
    "    # Group by CustomerID\n",
    "    rfm = df.groupby('CustomerID').agg({\n",
    "        'InvoiceDate': lambda x: (analysis_date - x.max()).days,  # Recency\n",
    "        'InvoiceNo': 'nunique',  # Frequency\n",
    "        'TotalValue': 'sum'  # Monetary\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    rfm.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']\n",
    "    \n",
    "    # Add additional metrics\n",
    "    customer_data = df.groupby('CustomerID').agg({\n",
    "        'Country': lambda x: x.iloc[0],  # Get the country of the customer\n",
    "        'StockCode': 'nunique',  # Number of unique products\n",
    "        'Quantity': 'sum'  # Total quantity\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Merge with RFM data\n",
    "    rfm = pd.merge(rfm, customer_data, on='CustomerID')\n",
    "    rfm.rename(columns={'StockCode': 'UniqueProducts', 'Quantity': 'TotalQuantity'}, inplace=True)\n",
    "    \n",
    "    # Calculate average transaction value\n",
    "    rfm['AvgTransactionValue'] = rfm['Monetary'] / rfm['Frequency']\n",
    "    \n",
    "    print(\"\\nRFM Metrics Calculated:\")\n",
    "    print(f\"- Customers analyzed: {len(rfm)}\")\n",
    "    print(f\"- Average Recency: {rfm['Recency'].mean():.2f} days\")\n",
    "    print(f\"- Average Frequency: {rfm['Frequency'].mean():.2f} transactions\")\n",
    "    print(f\"- Average Monetary: £{rfm['Monetary'].mean():.2f}\")\n",
    "    \n",
    "    return rfm\n",
    "\n",
    "# Create RFM scores\n",
    "def create_rfm_scores(rfm):\n",
    "    \"\"\"\n",
    "    Create RFM scores and segment customers.\n",
    "    \"\"\"\n",
    "    # Handle potential edge cases where qcut might fail due to duplicates\n",
    "    # For Recency, lower is better (more recent)\n",
    "    try:\n",
    "        rfm['R_Score'] = pd.qcut(rfm['Recency'], 4, labels=[4, 3, 2, 1])\n",
    "    except ValueError:\n",
    "        # If pandas.qcut fails due to duplicate values, use rank instead\n",
    "        rfm['R_Score'] = pd.cut(rfm['Recency'], \n",
    "                                bins=[rfm['Recency'].min()-1, \n",
    "                                      rfm['Recency'].quantile(0.25),\n",
    "                                      rfm['Recency'].quantile(0.5),\n",
    "                                      rfm['Recency'].quantile(0.75),\n",
    "                                      rfm['Recency'].max()],\n",
    "                                labels=[4, 3, 2, 1])\n",
    "    \n",
    "    # For Frequency and Monetary, higher is better\n",
    "    try:\n",
    "        rfm['F_Score'] = pd.qcut(rfm['Frequency'], 4, labels=[1, 2, 3, 4])\n",
    "    except ValueError:\n",
    "        rfm['F_Score'] = pd.cut(rfm['Frequency'],\n",
    "                                bins=[rfm['Frequency'].min()-1,\n",
    "                                      rfm['Frequency'].quantile(0.25),\n",
    "                                      rfm['Frequency'].quantile(0.5),\n",
    "                                      rfm['Frequency'].quantile(0.75),\n",
    "                                      rfm['Frequency'].max()],\n",
    "                                labels=[1, 2, 3, 4])\n",
    "    \n",
    "    try:\n",
    "        rfm['M_Score'] = pd.qcut(rfm['Monetary'], 4, labels=[1, 2, 3, 4])\n",
    "    except ValueError:\n",
    "        rfm['M_Score'] = pd.cut(rfm['Monetary'],\n",
    "                                bins=[rfm['Monetary'].min()-1,\n",
    "                                      rfm['Monetary'].quantile(0.25),\n",
    "                                      rfm['Monetary'].quantile(0.5),\n",
    "                                      rfm['Monetary'].quantile(0.75),\n",
    "                                      rfm['Monetary'].max()],\n",
    "                                labels=[1, 2, 3, 4])\n",
    "    \n",
    "    # Calculate RFM score\n",
    "    rfm['RFM_Score'] = rfm['R_Score'].astype(str) + rfm['F_Score'].astype(str) + rfm['M_Score'].astype(str)\n",
    "    rfm['RFM_Total'] = rfm['R_Score'].astype(int) + rfm['F_Score'].astype(int) + rfm['M_Score'].astype(int)\n",
    "    \n",
    "    print(\"\\nRFM Scoring Complete:\")\n",
    "    print(\"- R_Score 4 = Most recent customers\")\n",
    "    print(\"- F_Score 4 = Most frequent customers\")\n",
    "    print(\"- M_Score 4 = Highest spending customers\")\n",
    "    \n",
    "    return rfm\n",
    "\n",
    "# Assign customer segments\n",
    "def assign_segments(rfm):\n",
    "    \"\"\"\n",
    "    Assign customers to segments based on RFM scores.\n",
    "    \"\"\"\n",
    "    # Define segments based on RFM scores\n",
    "    def segment_customer(row):\n",
    "        r, f, m = row['R_Score'], row['F_Score'], row['M_Score']\n",
    "        \n",
    "        # Convert to int for easier comparison\n",
    "        r, f, m = int(r), int(f), int(m)\n",
    "        \n",
    "        # Champions: recent, frequent buyers with high spend\n",
    "        if r >= 4 and (f >= 3 or m >= 3):\n",
    "            return 'Champions'\n",
    "        \n",
    "        # Loyal Customers: buy regularly with above average spend\n",
    "        elif (r >= 3) and (f >= 3) and (m >= 3):\n",
    "            return 'Loyal Customers'\n",
    "        \n",
    "        # Potential Loyalists: recent customers with average frequency\n",
    "        elif r >= 4 and (f >= 2):\n",
    "            return 'Potential Loyalists'\n",
    "        \n",
    "        # Promising: recent customers, not frequent yet\n",
    "        elif r >= 3 and (f <= 2) and (m <= 2):\n",
    "            return 'Promising'\n",
    "        \n",
    "        # Needs Attention: above average recency, frequency and monetary values\n",
    "        elif (r >= 2) and (f >= 2) and (m >= 2):\n",
    "            return 'Needs Attention'\n",
    "        \n",
    "        # At Risk: purchased often but long time ago\n",
    "        elif (r <= 2) and (f >= 3) and (m >= 3):\n",
    "            return 'At Risk'\n",
    "        \n",
    "        # Can't Lose: made big purchases and often but haven't returned recently\n",
    "        elif (r <= 1) and (f >= 4) and (m >= 4):\n",
    "            return \"Can't Lose\"\n",
    "        \n",
    "        # Hibernating: low values in all metrics\n",
    "        elif (r <= 2) and (f <= 2):\n",
    "            return 'Hibernating'\n",
    "        \n",
    "        # Others\n",
    "        else:\n",
    "            return 'Others'\n",
    "    \n",
    "    rfm['Segment'] = rfm.apply(segment_customer, axis=1)\n",
    "    \n",
    "    segment_counts = rfm['Segment'].value_counts()\n",
    "    print(\"\\nCustomer Segments:\")\n",
    "    for segment, count in segment_counts.items():\n",
    "        print(f\"- {segment}: {count} customers ({count/len(rfm)*100:.2f}%)\")\n",
    "    \n",
    "    return rfm\n",
    "\n",
    "# Analyze segments\n",
    "def analyze_segments(rfm):\n",
    "    \"\"\"\n",
    "    Analyze characteristics of each segment.\n",
    "    \"\"\"\n",
    "    # Calculate average metrics per segment\n",
    "    segment_metrics = rfm.groupby('Segment').agg({\n",
    "        'Recency': 'mean',\n",
    "        'Frequency': 'mean',\n",
    "        'Monetary': 'mean',\n",
    "        'RFM_Total': 'mean',\n",
    "        'UniqueProducts': 'mean',\n",
    "        'AvgTransactionValue': 'mean',\n",
    "        'CustomerID': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    segment_metrics.rename(columns={'CustomerID': 'Count'}, inplace=True)\n",
    "    segment_metrics = segment_metrics.sort_values('RFM_Total', ascending=False)\n",
    "    \n",
    "    print(\"\\nSegment Profiles:\")\n",
    "    for _, row in segment_metrics.iterrows():\n",
    "        print(f\"\\n{row['Segment']} ({int(row['Count'])} customers):\")\n",
    "        print(f\"- Avg Recency: {row['Recency']:.2f} days\")\n",
    "        print(f\"- Avg Frequency: {row['Frequency']:.2f} transactions\")\n",
    "        print(f\"- Avg Monetary: £{row['Monetary']:.2f}\")\n",
    "        print(f\"- Avg RFM Score: {row['RFM_Total']:.2f}\")\n",
    "        print(f\"- Avg Unique Products: {row['UniqueProducts']:.2f}\")\n",
    "        print(f\"- Avg Transaction Value: £{row['AvgTransactionValue']:.2f}\")\n",
    "    \n",
    "    return segment_metrics\n",
    "\n",
    "# Create visualizations\n",
    "def create_visualizations(rfm, segment_metrics):\n",
    "    \"\"\"\n",
    "    Create visualizations for RFM analysis.\n",
    "    \"\"\"\n",
    "    # Set up figure\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # 1. Segment Distribution\n",
    "    plt.subplot(2, 2, 1)\n",
    "    ax1 = sns.countplot(y='Segment', data=rfm, order=rfm['Segment'].value_counts().index, \n",
    "                   palette='viridis')\n",
    "    plt.title('Customer Segment Distribution')\n",
    "    plt.xlabel('Number of Customers')\n",
    "    plt.ylabel('Segment')\n",
    "    \n",
    "    # Add count labels\n",
    "    for p in ax1.patches:\n",
    "        width = p.get_width()\n",
    "        plt.text(width + 1, p.get_y() + p.get_height()/2, f'{int(width)}', \n",
    "                va='center')\n",
    "    \n",
    "    # 2. Average Monetary Value by Segment\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sorted_segments = segment_metrics.sort_values('Monetary', ascending=False)\n",
    "    ax2 = sns.barplot(x='Segment', y='Monetary', data=sorted_segments, palette='viridis')\n",
    "    plt.title('Average Spend by Segment')\n",
    "    plt.xlabel('Segment')\n",
    "    plt.ylabel('Average Spend (£)')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for p in ax2.patches:\n",
    "        ax2.annotate(f'£{p.get_height():.0f}', \n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center', \n",
    "                    xytext=(0, 9), \n",
    "                    textcoords='offset points')\n",
    "    \n",
    "    # 3. RFM Heatmap\n",
    "    plt.subplot(2, 2, 3)\n",
    "    # Convert R_Score, F_Score to numeric if they're categorical\n",
    "    rfm_numeric = rfm.copy()\n",
    "    rfm_numeric['R_Score'] = rfm_numeric['R_Score'].astype(int)\n",
    "    rfm_numeric['F_Score'] = rfm_numeric['F_Score'].astype(int)\n",
    "    \n",
    "    # Create pivot table\n",
    "    rfm_pivot = rfm_numeric.pivot_table(\n",
    "        values='Monetary', \n",
    "        index=['R_Score'], \n",
    "        columns=['F_Score'], \n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(rfm_pivot, annot=True, fmt='.0f', cmap='viridis')\n",
    "    plt.title('Average Spend by Recency and Frequency Scores')\n",
    "    plt.xlabel('Frequency Score')\n",
    "    plt.ylabel('Recency Score')\n",
    "    \n",
    "    # # 4. Country Distribution by Segment\n",
    "    # plt.subplot(2, 2, 4)\n",
    "    # country_segment = pd.crosstab(rfm['Country'], rfm['Segment'])\n",
    "    # country_segment.plot(kind='bar', stacked=True, colormap='viridis')\n",
    "    # plt.title('Segments by Country')\n",
    "    # plt.xlabel('Country')\n",
    "    # plt.ylabel('Number of Customers')\n",
    "    # plt.xticks(rotation=45)\n",
    "    \n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig('rfm_analysis_results.png', dpi=300)\n",
    "    # print(\"\\nVisualizations have been saved as 'rfm_analysis_results.png'\")\n",
    "\n",
    "# Get top customers\n",
    "def get_top_customers(rfm, n=10):\n",
    "    \"\"\"\n",
    "    Identify top customers by RFM score.\n",
    "    \"\"\"\n",
    "    top_customers = rfm.sort_values(['RFM_Total', 'Monetary'], ascending=False).head(n)\n",
    "    \n",
    "    print(f\"\\nTop {n} Customers:\")\n",
    "    for i, (_, customer) in enumerate(top_customers.iterrows(), 1):\n",
    "        print(f\"\\n{i}. Customer ID: {int(customer['CustomerID'])}\")\n",
    "        print(f\"   - Country: {customer['Country']}\")\n",
    "        print(f\"   - Segment: {customer['Segment']}\")\n",
    "        print(f\"   - RFM Score: {int(customer['RFM_Total'])}/12\")\n",
    "        print(f\"   - Total Spend: £{customer['Monetary']:.2f}\")\n",
    "        print(f\"   - Purchase Frequency: {customer['Frequency']} transactions\")\n",
    "        print(f\"   - Unique Products: {customer['UniqueProducts']}\")\n",
    "    \n",
    "    return top_customers\n",
    "\n",
    "# Main function to run the complete analysis\n",
    "# At the end of your existing code, right before the \"if __name__ == '__main__':\" line,\n",
    "# add the alternative visualization functions from the artifact I provided.\n",
    "\n",
    "# Then, modify your run_rfm_analysis function to include the new visualizations:\n",
    "\n",
    "def run_rfm_analysis(file_path, analysis_date=None):\n",
    "    \"\"\"\n",
    "    Run the complete RFM analysis workflow.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"RFM CUSTOMER SEGMENTATION ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    df = load_and_prepare_data(file_path)\n",
    "    \n",
    "    # Perform RFM analysis\n",
    "    rfm = perform_rfm_analysis(df, analysis_date)\n",
    "    \n",
    "    # Create RFM scores\n",
    "    rfm = create_rfm_scores(rfm)\n",
    "    \n",
    "    # Assign segments\n",
    "    rfm = assign_segments(rfm)\n",
    "    \n",
    "    # Analyze segments\n",
    "    segment_metrics = analyze_segments(rfm)\n",
    "    \n",
    "    # Create standard visualizations\n",
    "    create_visualizations(rfm, segment_metrics)\n",
    "    \n",
    "    # Create additional country-based visualizations\n",
    "    print(\"\\nGenerating alternative country visualizations...\")\n",
    "    \n",
    "    # Get top customers\n",
    "    top_customers = get_top_customers(rfm, 5)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return rfm, segment_metrics, top_customers\n",
    "\n",
    "# If running as a script\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the analysis\n",
    "    file_path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx\"  # Update with your file path\n",
    "    rfm_results, segment_metrics, top_customers = run_rfm_analysis(file_path)\n",
    "    \n",
    "    # Export results to CSV\n",
    "    rfm_results.to_csv('data/rfm_customer_segments.csv', index=False)\n",
    "    segment_metrics.to_csv('data/segment_profiles.csv', index=False)\n",
    "    \n",
    "    print(\"\\nResults exported to CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. K-means Clustering (machine-learning-based)\n",
    "\n",
    "K-Means Clustering is an unsupervised machine learning algorithm used to group similar data points into K clusters. It is widely used in customer segmentation, anomaly detection, and pattern recognition.\n",
    "\n",
    "#### How It Works\n",
    "1. Choose K (number of clusters).\n",
    "2. Randomly initialize K centroids.\n",
    "3. Assign each data point to the nearest centroid.\n",
    "4. Update centroids based on the mean of assigned points.\n",
    "5. Repeat until centroids stabilize or reach a stopping criterion.\n",
    "\n",
    "#### Why Use K-Means for Customer Segmentation?\n",
    "* Automatically groups customers with similar behaviors.\n",
    "* Scalable and efficient for large datasets.\n",
    "* Helps identify patterns such as high-value customers, frequent shoppers, or churn risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_theme()\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "def load_and_prepare_data(file_path):\n",
    "    \"\"\"\n",
    "    Load Excel file and prepare data for clustering.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    print(\"\\nDataset Overview:\")\n",
    "    print(f\"- Shape: {df.shape}\")\n",
    "    print(f\"- Columns: {', '.join(df.columns)}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"\\nMissing Values:\")\n",
    "    for col, count in missing_values.items():\n",
    "        if count > 0:\n",
    "            print(f\"- {col}: {count} missing values ({count/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Filter out records with missing CustomerID\n",
    "    df_clean = df.dropna(subset=['CustomerID'])\n",
    "    df_clean['CustomerID'] = df_clean['CustomerID'].astype(int)\n",
    "    \n",
    "    # Check for negative quantities (returns)\n",
    "    returns = df_clean[df_clean['Quantity'] < 0]\n",
    "    print(f\"\\nReturns identified: {len(returns)} records\")\n",
    "    \n",
    "    # Calculate total value of each transaction\n",
    "    df_clean['TotalValue'] = df_clean['Quantity'] * df_clean['UnitPrice']\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def calculate_customer_features(df, analysis_date=None):\n",
    "    \"\"\"\n",
    "    Calculate features for each customer for clustering.\n",
    "    \"\"\"\n",
    "    # If no analysis date provided, use the max date in the dataset + 1 day\n",
    "    if analysis_date is None:\n",
    "        analysis_date = df['InvoiceDate'].max() + timedelta(days=1)\n",
    "        print(f\"Using analysis date: {analysis_date}\")\n",
    "    \n",
    "    # RFM features\n",
    "    rfm = df.groupby('CustomerID').agg({\n",
    "        'InvoiceDate': lambda x: (analysis_date - x.max()).days,  # Recency\n",
    "        'InvoiceNo': 'nunique',  # Frequency\n",
    "        'TotalValue': 'sum'  # Monetary\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    rfm.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']\n",
    "    \n",
    "    # Add additional features\n",
    "    customer_features = df.groupby('CustomerID').agg({\n",
    "        'Country': lambda x: x.iloc[0],  # Country\n",
    "        'StockCode': 'nunique',  # Product variety\n",
    "        'Quantity': 'sum',  # Total quantity\n",
    "        'InvoiceDate': ['min', 'max']  # First and last purchase dates\n",
    "    })\n",
    "    \n",
    "    # Flatten multi-level columns\n",
    "    customer_features.columns = ['Country', 'UniqueProducts', 'TotalQuantity', 'FirstPurchase', 'LastPurchase']\n",
    "    customer_features = customer_features.reset_index()\n",
    "    \n",
    "    # Merge with RFM data\n",
    "    customer_data = pd.merge(rfm, customer_features, on='CustomerID')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    customer_data['AvgOrderValue'] = customer_data['Monetary'] / customer_data['Frequency']\n",
    "    customer_data['AvgQuantityPerOrder'] = customer_data['TotalQuantity'] / customer_data['Frequency']\n",
    "    customer_data['AvgUniqueProductsPerOrder'] = customer_data['UniqueProducts'] / customer_data['Frequency']\n",
    "    customer_data['PurchaseSpan'] = (customer_data['LastPurchase'] - customer_data['FirstPurchase']).dt.days\n",
    "    \n",
    "    # Handle customers with only one purchase (PurchaseSpan = 0)\n",
    "    customer_data['PurchaseSpan'] = customer_data['PurchaseSpan'].replace(0, 1)\n",
    "    \n",
    "    # Calculate purchase frequency (purchases per day in the active period)\n",
    "    customer_data['PurchaseFrequency'] = customer_data['Frequency'] / customer_data['PurchaseSpan']\n",
    "    \n",
    "    # Calculate days since first purchase\n",
    "    customer_data['CustomerAge'] = (analysis_date - customer_data['FirstPurchase']).dt.days\n",
    "    \n",
    "    print(\"\\nCustomer Features Calculated:\")\n",
    "    print(f\"- Customers analyzed: {len(customer_data)}\")\n",
    "    print(f\"- Features created: {len(customer_data.columns) - 2}\")  # Exclude CustomerID and Country\n",
    "    \n",
    "    return customer_data\n",
    "\n",
    "def prepare_features_for_clustering(customer_data):\n",
    "    \"\"\"\n",
    "    Prepare customer features for clustering by selecting and scaling features.\n",
    "    \"\"\"\n",
    "    # Select features for clustering\n",
    "    features = [\n",
    "        'Recency', 'Frequency', 'Monetary', \n",
    "        'UniqueProducts', 'TotalQuantity',\n",
    "        'AvgOrderValue', 'AvgQuantityPerOrder', 'AvgUniqueProductsPerOrder',\n",
    "        'PurchaseFrequency', 'CustomerAge'\n",
    "    ]\n",
    "    \n",
    "    # Create a copy of the data with selected features\n",
    "    cluster_data = customer_data[['CustomerID', 'Country'] + features].copy()\n",
    "    \n",
    "    # Remove outliers (optional)\n",
    "    # Here using simple approach: keep rows where all values are within 3 std of mean\n",
    "    for feature in features:\n",
    "        mean = cluster_data[feature].mean()\n",
    "        std = cluster_data[feature].std()\n",
    "        cluster_data = cluster_data[\n",
    "            (cluster_data[feature] <= mean + 3 * std) & \n",
    "            (cluster_data[feature] >= mean - 3 * std)\n",
    "        ]\n",
    "    \n",
    "    print(f\"\\nAfter outlier removal: {len(cluster_data)} customers\")\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    cluster_data_scaled = scaler.fit_transform(cluster_data[features])\n",
    "    \n",
    "    # Create a DataFrame with scaled values\n",
    "    cluster_data_scaled_df = pd.DataFrame(\n",
    "        cluster_data_scaled, \n",
    "        columns=features, \n",
    "        index=cluster_data.index\n",
    "    )\n",
    "    \n",
    "    return cluster_data, cluster_data_scaled_df, features, scaler\n",
    "\n",
    "def determine_optimal_clusters(data_scaled, max_clusters=10):\n",
    "    \"\"\"\n",
    "    Determine the optimal number of clusters using the elbow method and silhouette score.\n",
    "    \"\"\"\n",
    "    # Calculate inertia (sum of squared distances to closest centroid)\n",
    "    inertia = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for n_clusters in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        kmeans.fit(data_scaled)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "        \n",
    "        # Calculate silhouette score\n",
    "        silhouette_avg = silhouette_score(data_scaled, kmeans.labels_)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "    \n",
    "    # Plot elbow method\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(2, max_clusters + 1), inertia, marker='o')\n",
    "    plt.title('Elbow Method for Optimal Clusters')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Inertia (Sum of Squared Distances)')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(2, max_clusters + 1), silhouette_scores, marker='o')\n",
    "    plt.title('Silhouette Score for Optimal Clusters')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/optimal_clusters.png', dpi=300)\n",
    "    \n",
    "    # Find optimal number of clusters\n",
    "    # Simple approach: find the cluster number corresponding to maximum silhouette score\n",
    "    optimal_clusters = np.argmax(silhouette_scores) + 2  # +2 because we started from 2 clusters\n",
    "    \n",
    "    print(f\"\\nOptimal number of clusters based on Silhouette Score: {optimal_clusters}\")\n",
    "    print(f\"Silhouette Score for {optimal_clusters} clusters: {silhouette_scores[optimal_clusters-2]:.4f}\")\n",
    "    \n",
    "    return optimal_clusters\n",
    "\n",
    "def perform_clustering(data_scaled, n_clusters, cluster_data, features):\n",
    "    \"\"\"\n",
    "    Perform KMeans clustering with the optimal number of clusters.\n",
    "    \"\"\"\n",
    "    # Fit KMeans with optimal number of clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    kmeans.fit(data_scaled)\n",
    "    \n",
    "    # Add cluster labels to original data\n",
    "    cluster_data['Cluster'] = kmeans.labels_\n",
    "    \n",
    "    # Calculate cluster centers for original features\n",
    "    cluster_centers = pd.DataFrame(kmeans.cluster_centers_, columns=features)\n",
    "    \n",
    "    print(\"\\nCluster Centers:\")\n",
    "    print(cluster_centers)\n",
    "    \n",
    "    return cluster_data, cluster_centers, kmeans\n",
    "\n",
    "def visualize_clusters(cluster_data, features, kmeans, scaler):\n",
    "    \"\"\"\n",
    "    Visualize clusters using PCA for dimensionality reduction.\n",
    "    \"\"\"\n",
    "    # Use PCA to reduce dimensions for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    principal_components = pca.fit_transform(cluster_data[features])\n",
    "    \n",
    "    # Create a DataFrame with principal components\n",
    "    pca_df = pd.DataFrame(\n",
    "        data=principal_components, \n",
    "        columns=['PC1', 'PC2']\n",
    "    )\n",
    "    pca_df['Cluster'] = kmeans.labels_\n",
    "    pca_df['CustomerID'] = cluster_data['CustomerID'].values\n",
    "    \n",
    "    # Calculate variance explained by principal components\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    \n",
    "    # Plot PCA results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot each cluster\n",
    "    for cluster in range(kmeans.n_clusters):\n",
    "        cluster_data = pca_df[pca_df['Cluster'] == cluster]\n",
    "        plt.scatter(\n",
    "            cluster_data['PC1'], \n",
    "            cluster_data['PC2'], \n",
    "            label=f'Cluster {cluster}',\n",
    "            s=50, alpha=0.7\n",
    "        )\n",
    "    \n",
    "    # Add cluster centers\n",
    "    centers_pca = pca.transform(kmeans.cluster_centers_)\n",
    "    plt.scatter(\n",
    "        centers_pca[:, 0], centers_pca[:, 1],\n",
    "        s=200, marker='X', c='black', label='Centroids'\n",
    "    )\n",
    "    \n",
    "    plt.title('Customer Segments Visualization using PCA', fontsize=14)\n",
    "    plt.xlabel(f'Principal Component 1 ({explained_variance[0]:.2%} variance)', fontsize=12)\n",
    "    plt.ylabel(f'Principal Component 2 ({explained_variance[1]:.2%} variance)', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/customer_clusters_pca.png', dpi=300)\n",
    "    print(\"Clusters visualization saved as 'customer_clusters_pca.png'\")\n",
    "    \n",
    "    # Feature importance for principal components\n",
    "    feature_importance = pd.DataFrame(\n",
    "        pca.components_.T,\n",
    "        columns=['PC1', 'PC2'],\n",
    "        index=features\n",
    "    )\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        feature_importance, \n",
    "        annot=True, \n",
    "        cmap='viridis', \n",
    "        center=0,\n",
    "        linewidths=.5,\n",
    "        cbar_kws={'label': 'Contribution to Principal Component'}\n",
    "    )\n",
    "    \n",
    "    plt.title('Feature Contributions to Principal Components', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/pca_feature_importance.png', dpi=300)\n",
    "    print(\"PCA feature importance saved as 'pca_feature_importance.png'\")\n",
    "    \n",
    "    return pca_df\n",
    "\n",
    "def analyze_clusters(cluster_data, n_clusters):\n",
    "    \"\"\"\n",
    "    Analyze and characterize each cluster.\n",
    "    \"\"\"\n",
    "    # Calculate cluster statistics\n",
    "    cluster_stats = cluster_data.groupby('Cluster').agg({\n",
    "        'CustomerID': 'count',\n",
    "        'Recency': 'mean',\n",
    "        'Frequency': 'mean',\n",
    "        'Monetary': 'mean',\n",
    "        'UniqueProducts': 'mean',\n",
    "        'AvgOrderValue': 'mean',\n",
    "        'PurchaseFrequency': 'mean',\n",
    "        'CustomerAge': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Rename columns for better readability\n",
    "    cluster_stats.rename(columns={'CustomerID': 'Count'}, inplace=True)\n",
    "    \n",
    "    # Add percentage of customers\n",
    "    total_customers = cluster_stats['Count'].sum()\n",
    "    cluster_stats['Percentage'] = (cluster_stats['Count'] / total_customers * 100).round(2)\n",
    "    \n",
    "    # Calculate average revenue per customer\n",
    "    cluster_stats['AvgRevenuePerCustomer'] = cluster_stats['Monetary']\n",
    "    \n",
    "    # Sort by average revenue per customer (descending)\n",
    "    cluster_stats = cluster_stats.sort_values('AvgRevenuePerCustomer', ascending=False)\n",
    "    \n",
    "    print(\"\\nCluster Statistics:\")\n",
    "    print(cluster_stats)\n",
    "    \n",
    "    # Characterize clusters based on statistics\n",
    "    cluster_characteristics = []\n",
    "    \n",
    "    for _, row in cluster_stats.iterrows():\n",
    "        cluster = row['Cluster']\n",
    "        characteristics = {}\n",
    "        \n",
    "        # Recency characterization\n",
    "        if row['Recency'] <= cluster_stats['Recency'].quantile(0.25):\n",
    "            characteristics['Recency'] = 'Very Recent'\n",
    "        elif row['Recency'] <= cluster_stats['Recency'].quantile(0.5):\n",
    "            characteristics['Recency'] = 'Recent'\n",
    "        elif row['Recency'] <= cluster_stats['Recency'].quantile(0.75):\n",
    "            characteristics['Recency'] = 'Not Recent'\n",
    "        else:\n",
    "            characteristics['Recency'] = 'Inactive'\n",
    "        \n",
    "        # Frequency characterization\n",
    "        if row['Frequency'] >= cluster_stats['Frequency'].quantile(0.75):\n",
    "            characteristics['Frequency'] = 'Very Frequent'\n",
    "        elif row['Frequency'] >= cluster_stats['Frequency'].quantile(0.5):\n",
    "            characteristics['Frequency'] = 'Frequent'\n",
    "        elif row['Frequency'] >= cluster_stats['Frequency'].quantile(0.25):\n",
    "            characteristics['Frequency'] = 'Occasional'\n",
    "        else:\n",
    "            characteristics['Frequency'] = 'Rare'\n",
    "        \n",
    "        # Monetary characterization\n",
    "        if row['Monetary'] >= cluster_stats['Monetary'].quantile(0.75):\n",
    "            characteristics['Monetary'] = 'High Spender'\n",
    "        elif row['Monetary'] >= cluster_stats['Monetary'].quantile(0.5):\n",
    "            characteristics['Monetary'] = 'Medium Spender'\n",
    "        elif row['Monetary'] >= cluster_stats['Monetary'].quantile(0.25):\n",
    "            characteristics['Monetary'] = 'Low Spender'\n",
    "        else:\n",
    "            characteristics['Monetary'] = 'Very Low Spender'\n",
    "        \n",
    "        # Product variety characterization\n",
    "        if row['UniqueProducts'] >= cluster_stats['UniqueProducts'].quantile(0.75):\n",
    "            characteristics['Product Variety'] = 'Very Diverse'\n",
    "        elif row['UniqueProducts'] >= cluster_stats['UniqueProducts'].quantile(0.5):\n",
    "            characteristics['Product Variety'] = 'Diverse'\n",
    "        elif row['UniqueProducts'] >= cluster_stats['UniqueProducts'].quantile(0.25):\n",
    "            characteristics['Product Variety'] = 'Somewhat Limited'\n",
    "        else:\n",
    "            characteristics['Product Variety'] = 'Limited'\n",
    "        \n",
    "        # Customer age characterization\n",
    "        if row['CustomerAge'] >= cluster_stats['CustomerAge'].quantile(0.75):\n",
    "            characteristics['Customer Age'] = 'Long-term'\n",
    "        elif row['CustomerAge'] >= cluster_stats['CustomerAge'].quantile(0.5):\n",
    "            characteristics['Customer Age'] = 'Established'\n",
    "        elif row['CustomerAge'] >= cluster_stats['CustomerAge'].quantile(0.25):\n",
    "            characteristics['Customer Age'] = 'Recent'\n",
    "        else:\n",
    "            characteristics['Customer Age'] = 'New'\n",
    "        \n",
    "        # Suggested segment name based on characteristics\n",
    "        segment_name = generate_segment_name(characteristics)\n",
    "        \n",
    "        cluster_characteristics.append({\n",
    "            'Cluster': cluster,\n",
    "            'Count': row['Count'],\n",
    "            'Percentage': row['Percentage'],\n",
    "            'AvgRevenuePerCustomer': row['AvgRevenuePerCustomer'],\n",
    "            'Characteristics': characteristics,\n",
    "            'Segment Name': segment_name\n",
    "        })\n",
    "    \n",
    "    # Print cluster characteristics\n",
    "    print(\"\\nCluster Characterization:\")\n",
    "    for cluster in cluster_characteristics:\n",
    "        print(f\"\\nCluster {cluster['Cluster']} ({cluster['Count']} customers, {cluster['Percentage']}%):\")\n",
    "        print(f\"Segment Name: {cluster['Segment Name']}\")\n",
    "        print(f\"Average Revenue: £{cluster['AvgRevenuePerCustomer']:.2f}\")\n",
    "        for key, value in cluster['Characteristics'].items():\n",
    "            print(f\"- {key}: {value}\")\n",
    "    \n",
    "    return cluster_stats, cluster_characteristics\n",
    "\n",
    "def generate_segment_name(characteristics):\n",
    "    \"\"\"\n",
    "    Generate a meaningful segment name based on characteristics.\n",
    "    \"\"\"\n",
    "    # High-value combination\n",
    "    if (characteristics['Recency'] in ['Very Recent', 'Recent'] and \n",
    "        characteristics['Frequency'] in ['Very Frequent', 'Frequent'] and \n",
    "        characteristics['Monetary'] in ['High Spender']):\n",
    "        return \"VIP Customers\"\n",
    "    \n",
    "    # Loyal customers\n",
    "    elif (characteristics['Recency'] in ['Very Recent', 'Recent'] and \n",
    "          characteristics['Frequency'] in ['Very Frequent', 'Frequent'] and \n",
    "          characteristics['Monetary'] in ['Medium Spender']):\n",
    "        return \"Loyal Customers\"\n",
    "    \n",
    "    # Big spenders but not frequent\n",
    "    elif (characteristics['Monetary'] in ['High Spender'] and \n",
    "          characteristics['Frequency'] in ['Occasional', 'Rare']):\n",
    "        return \"Big Ticket Shoppers\"\n",
    "    \n",
    "    # Recent but not frequent or high spend\n",
    "    elif (characteristics['Recency'] in ['Very Recent', 'Recent'] and \n",
    "          characteristics['Frequency'] in ['Occasional', 'Rare']):\n",
    "        return \"New or Occasional Customers\"\n",
    "    \n",
    "    # Frequent but low spending\n",
    "    elif (characteristics['Frequency'] in ['Very Frequent', 'Frequent'] and \n",
    "          characteristics['Monetary'] in ['Low Spender', 'Very Low Spender']):\n",
    "        return \"Frequent Low Spenders\"\n",
    "    \n",
    "    # Not recent but previously good customers\n",
    "    elif (characteristics['Recency'] in ['Not Recent', 'Inactive'] and \n",
    "          characteristics['Monetary'] in ['High Spender', 'Medium Spender']):\n",
    "        return \"At-Risk High-Value Customers\"\n",
    "    \n",
    "    # Not recent and not high value\n",
    "    elif (characteristics['Recency'] in ['Not Recent', 'Inactive'] and \n",
    "          characteristics['Monetary'] in ['Low Spender', 'Very Low Spender']):\n",
    "        return \"Churned Customers\"\n",
    "    \n",
    "    # Diverse product selection\n",
    "    elif characteristics['Product Variety'] in ['Very Diverse', 'Diverse']:\n",
    "        return \"Product Explorers\"\n",
    "    \n",
    "    # Long-term relationship\n",
    "    elif characteristics['Customer Age'] in ['Long-term', 'Established']:\n",
    "        return \"Long-term Relationship Customers\"\n",
    "    \n",
    "    # Default case\n",
    "    else:\n",
    "        return \"General Customers\"\n",
    "\n",
    "def visualize_cluster_profiles(cluster_stats):\n",
    "    \"\"\"\n",
    "    Visualize the profiles of each cluster.\n",
    "    \"\"\"\n",
    "    # Prepare data for radar chart\n",
    "    features_for_radar = ['Recency', 'Frequency', 'Monetary', 'UniqueProducts', \n",
    "                          'AvgOrderValue', 'PurchaseFrequency', 'CustomerAge']\n",
    "    \n",
    "    # Scale features for radar chart\n",
    "    radar_df = cluster_stats[['Cluster'] + features_for_radar].copy()\n",
    "    \n",
    "    for feature in features_for_radar:\n",
    "        max_val = radar_df[feature].max()\n",
    "        min_val = radar_df[feature].min()\n",
    "        \n",
    "        # For recency, lower is better, so invert the scale\n",
    "        if feature == 'Recency':\n",
    "            radar_df[feature] = 1 - ((radar_df[feature] - min_val) / (max_val - min_val))\n",
    "        else:\n",
    "            radar_df[feature] = (radar_df[feature] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Create radar chart\n",
    "    n_clusters = len(radar_df)\n",
    "    n_features = len(features_for_radar)\n",
    "    \n",
    "    # Compute angles for radar chart\n",
    "    angles = np.linspace(0, 2*np.pi, n_features, endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Close the circle\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # Add feature labels\n",
    "    plt.xticks(angles[:-1], features_for_radar, fontsize=12)\n",
    "    \n",
    "    # Draw cluster profiles\n",
    "    for _, row in radar_df.iterrows():\n",
    "        values = row[features_for_radar].values.tolist()\n",
    "        values += values[:1]  # Close the polygon\n",
    "        \n",
    "        # Plot values\n",
    "        ax.plot(angles, values, linewidth=2, label=f\"Cluster {row['Cluster']}\")\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    \n",
    "    plt.title('Cluster Profiles Comparison', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/cluster_profiles_radar.png', dpi=300)\n",
    "    print(\"Cluster profiles radar chart saved as 'cluster_profiles_radar.png'\")\n",
    "    \n",
    "    # Create bar charts for key metrics\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # Select metrics to visualize\n",
    "    metrics = ['Count', 'Percentage', 'Recency', 'Frequency', 'Monetary', 'UniqueProducts', \n",
    "               'AvgOrderValue', 'PurchaseFrequency', 'CustomerAge']\n",
    "    \n",
    "    # Create subplots\n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        \n",
    "        # Sort by the current metric for better visualization\n",
    "        sorted_data = cluster_stats.sort_values(metric)\n",
    "        \n",
    "        # Create bar plot\n",
    "        ax = sns.barplot(x='Cluster', y=metric, data=sorted_data, palette='viridis')\n",
    "        \n",
    "        # Add value labels\n",
    "        for p in ax.patches:\n",
    "            height = p.get_height()\n",
    "            ax.text(p.get_x() + p.get_width()/2., height + 0.01,\n",
    "                   f'{height:.1f}' if height < 1000 else f'{int(height)}',\n",
    "                   ha=\"center\", fontsize=9)\n",
    "        \n",
    "        plt.title(f'{metric} by Cluster')\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    plt.suptitle('Key Metrics by Cluster', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/cluster_metrics.png', dpi=300)\n",
    "    print(\"Cluster metrics visualization saved as 'cluster_metrics.png'\")\n",
    "\n",
    "def analyze_country_distribution(cluster_data):\n",
    "    \"\"\"\n",
    "    Analyze the country distribution within each cluster.\n",
    "    \"\"\"\n",
    "    # Cross-tabulation of clusters and countries\n",
    "    country_cluster = pd.crosstab(\n",
    "        cluster_data['Country'], \n",
    "        cluster_data['Cluster'],\n",
    "        normalize='columns'\n",
    "    ) * 100\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        country_cluster, \n",
    "        annot=True, \n",
    "        fmt='.1f', \n",
    "        cmap='viridis',\n",
    "        linewidths=.5,\n",
    "        cbar_kws={'label': 'Percentage in Cluster'}\n",
    "    )\n",
    "    plt.title('Country Distribution by Cluster (%)', fontsize=14)\n",
    "    plt.xlabel('Cluster', fontsize=12)\n",
    "    plt.ylabel('Country', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/country_cluster_distribution.png', dpi=300)\n",
    "    print(\"Country distribution by cluster saved as 'country_cluster_distribution.png'\")\n",
    "    \n",
    "    return country_cluster\n",
    "\n",
    "def run_ml_segmentation(file_path, analysis_date=None):\n",
    "    \"\"\"\n",
    "    Run the complete machine learning-based customer segmentation workflow.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"MACHINE LEARNING-BASED CUSTOMER SEGMENTATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    df = load_and_prepare_data(file_path)\n",
    "    \n",
    "    # Calculate customer features\n",
    "    customer_data = calculate_customer_features(df, analysis_date)\n",
    "    \n",
    "    # Prepare features for clustering\n",
    "    cluster_data, cluster_data_scaled, features, scaler = prepare_features_for_clustering(customer_data)\n",
    "    \n",
    "    # Determine optimal number of clusters\n",
    "    n_clusters = determine_optimal_clusters(cluster_data_scaled, max_clusters=10)\n",
    "    \n",
    "    # Perform clustering\n",
    "    clustered_data, cluster_centers, kmeans = perform_clustering(\n",
    "        cluster_data_scaled, n_clusters, cluster_data, features\n",
    "    )\n",
    "    \n",
    "    # Visualize clusters\n",
    "    pca_df = visualize_clusters(clustered_data, features, kmeans, scaler)\n",
    "    \n",
    "    # Analyze clusters\n",
    "    cluster_stats, cluster_characteristics = analyze_clusters(clustered_data, n_clusters)\n",
    "    \n",
    "    # Visualize cluster profiles\n",
    "    visualize_cluster_profiles(cluster_stats)\n",
    "    \n",
    "    # Analyze country distribution\n",
    "    country_cluster = analyze_country_distribution(clustered_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Export results to CSV\n",
    "    clustered_data.to_csv('data/ml_customer_segments.csv', index=False)\n",
    "    cluster_stats.to_csv('data/ml_segment_profiles.csv', index=False)\n",
    "    \n",
    "    print(\"\\nResults exported to CSV files.\")\n",
    "    \n",
    "    return clustered_data, cluster_stats, cluster_characteristics\n",
    "\n",
    "# If running as a script\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the analysis\n",
    "    file_path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx\"  # Update with your file path\n",
    "    clustered_data, cluster_stats, cluster_characteristics = run_ml_segmentation(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Purchase Patterns/Association Rule Mining\n",
    "### a. Apriori Algorithm (non-machine-learning)\n",
    "\n",
    "The Apriori Algorithm is a rule-based approach for market basket analysis, helping businesses understand customer purchase patterns by identifying items frequently bought together. It is widely used in cross-selling, product recommendations, and inventory optimization.\n",
    "\n",
    "#### How the Apriori Algorithm Works\n",
    "1. Identify Frequent Itemsets\n",
    "2. Scan transaction data to find items frequently purchased together based on a minimum support threshold.\n",
    "3. Generate Association Rules\n",
    "4. Create if-then rules (e.g., If a customer buys A, they are likely to buy B).\n",
    "5. Evaluate Rule Strength\n",
    "6. Use metrics like Support, Confidence, and Lift to measure rule significance:\n",
    "    * Support: How often an itemset appears in transactions.\n",
    "    * Confidence: Likelihood of item B being bought when item A is purchased.\n",
    "    * Lift: Whether A and B occur together more than expected by chance.\n",
    "> Example Rule:\n",
    ">> \"If a customer buys bread and butter, they are 70% likely to also buy milk.\"\n",
    "\n",
    "#### Why Use Apriori?\n",
    "* Simple and interpretable for business decision-making.\n",
    "* Improves product bundling and recommendation strategies.\n",
    "* Enhances store layout planning to increase sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mlxtend\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_theme()\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "def load_retail_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and prepare the retail dataset for association rule mining.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    print(\"\\nDataset Overview:\")\n",
    "    print(f\"- Shape: {df.shape}\")\n",
    "    print(f\"- Columns: {', '.join(df.columns)}\")\n",
    "    \n",
    "    # Basic data cleaning\n",
    "    # Remove canceled orders (negative quantities)\n",
    "    df = df[df['Quantity'] > 0]\n",
    "    \n",
    "    # Remove missing descriptions or stock codes\n",
    "    df = df.dropna(subset=['Description', 'StockCode'])\n",
    "    \n",
    "    # Filter out non-product items if needed (e.g., shipping costs, adjustments)\n",
    "    # This is dataset specific and might need adjustment\n",
    "    non_product_codes = ['POST', 'D', 'M', 'CRUK', 'C2']\n",
    "    df = df[~df['StockCode'].isin(non_product_codes)]\n",
    "    \n",
    "    # Filter out items with generic descriptions\n",
    "    generic_descriptions = ['DISCOUNT', 'POSTAGE', 'MANUAL', 'SAMPLES', 'TEST']\n",
    "    df = df[~df['Description'].str.contains('|'.join(generic_descriptions), case=False)]\n",
    "    \n",
    "    print(f\"After cleaning: {df.shape[0]} transactions\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_transactions(df):\n",
    "    \"\"\"\n",
    "    Transform the retail data into a transaction format suitable for association rule mining.\n",
    "    \"\"\"\n",
    "    # Group items by invoice to create transactions\n",
    "    transactions = df.groupby(['InvoiceNo', 'Description'])['Quantity'].sum().reset_index()\n",
    "    \n",
    "    # Create a list of lists (each inner list is a transaction)\n",
    "    transaction_lists = transactions.groupby('InvoiceNo')['Description'].apply(list).tolist()\n",
    "    \n",
    "    # Print transaction statistics\n",
    "    print(f\"\\nTotal number of transactions: {len(transaction_lists)}\")\n",
    "    print(f\"Average number of items per transaction: {np.mean([len(t) for t in transaction_lists]):.2f}\")\n",
    "    print(f\"Max items in a transaction: {max([len(t) for t in transaction_lists])}\")\n",
    "    \n",
    "    return transaction_lists\n",
    "\n",
    "def create_one_hot_encoded_df(transaction_lists):\n",
    "    \"\"\"\n",
    "    Create a one-hot encoded DataFrame from transaction lists.\n",
    "    \"\"\"\n",
    "    # Use TransactionEncoder to convert transaction lists to a one-hot encoded format\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit_transform(transaction_lists)\n",
    "    one_hot_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    \n",
    "    print(f\"One-hot encoded data shape: {one_hot_df.shape}\")\n",
    "    \n",
    "    return one_hot_df\n",
    "\n",
    "def find_frequent_itemsets(one_hot_df, min_support=0.01):\n",
    "    \"\"\"\n",
    "    Find frequent itemsets using the Apriori algorithm.\n",
    "    \"\"\"\n",
    "    print(f\"\\nFinding frequent itemsets (min_support={min_support})...\")\n",
    "    \n",
    "    # Apply Apriori algorithm to find frequent itemsets\n",
    "    frequent_itemsets = apriori(one_hot_df, \n",
    "                               min_support=min_support, \n",
    "                               use_colnames=True,\n",
    "                               max_len=4)  # Limit to itemsets with up to 4 items\n",
    "    \n",
    "    print(f\"Found {len(frequent_itemsets)} frequent itemsets\")\n",
    "    \n",
    "    # Print top itemsets by support\n",
    "    print(\"\\nTop 10 frequent itemsets by support:\")\n",
    "    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "    \n",
    "    # Sort by support and length\n",
    "    sorted_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, True])\n",
    "    for i, (_, row) in enumerate(sorted_itemsets.head(10).iterrows()):\n",
    "        print(f\"{i+1}. {list(row['itemsets'])} - Support: {row['support']:.4f}\")\n",
    "    \n",
    "    return frequent_itemsets\n",
    "\n",
    "def generate_association_rules(frequent_itemsets, min_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Generate association rules from frequent itemsets.\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating association rules (min_confidence={min_threshold})...\")\n",
    "    \n",
    "    # Generate association rules\n",
    "    rules = association_rules(frequent_itemsets, \n",
    "                             metric=\"confidence\", \n",
    "                             min_threshold=min_threshold)\n",
    "    \n",
    "    # Add count columns for easier interpretation\n",
    "    rules['antecedent_len'] = rules['antecedents'].apply(lambda x: len(x))\n",
    "    rules['consequent_len'] = rules['consequents'].apply(lambda x: len(x))\n",
    "    \n",
    "    print(f\"Generated {len(rules)} rules\")\n",
    "    \n",
    "    # Sort rules by lift (higher lift indicates stronger association)\n",
    "    sorted_rules = rules.sort_values('lift', ascending=False)\n",
    "    \n",
    "    # Print top rules by lift\n",
    "    print(\"\\nTop 10 rules by lift:\")\n",
    "    for i, (_, row) in enumerate(sorted_rules.head(10).iterrows()):\n",
    "        antecedents = list(row['antecedents'])\n",
    "        consequents = list(row['consequents'])\n",
    "        print(f\"{i+1}. {antecedents} → {consequents}\")\n",
    "        print(f\"   Support: {row['support']:.4f}, Confidence: {row['confidence']:.4f}, Lift: {row['lift']:.4f}\")\n",
    "    \n",
    "    return rules\n",
    "\n",
    "def visualize_association_rules(rules, top_n=20):\n",
    "    \"\"\"\n",
    "    Create visualizations for the association rules.\n",
    "    \"\"\"\n",
    "    # Scatter plot of lift vs support, colored by confidence\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(rules['support'], rules['lift'], alpha=0.5, \n",
    "               c=rules['confidence'], cmap='viridis', s=rules['confidence']*100)\n",
    "    \n",
    "    plt.colorbar(label='Confidence')\n",
    "    plt.xlabel('Support')\n",
    "    plt.ylabel('Lift')\n",
    "    plt.title('Association Rules: Support vs Lift')\n",
    "    \n",
    "    # Add annotations for top rules by lift\n",
    "    top_rules = rules.sort_values('lift', ascending=False).head(10)\n",
    "    for i, (_, rule) in enumerate(top_rules.iterrows()):\n",
    "        plt.annotate(f\"{i+1}\", \n",
    "                    (rule['support'], rule['lift']),\n",
    "                    xytext=(5, 5),\n",
    "                    textcoords='offset points')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/association_rules_scatter.png', dpi=300)\n",
    "    print(\"Association rules scatter plot saved as 'figure/association_rules_scatter.png'\")\n",
    "    \n",
    "    # Network visualization of top rules\n",
    "    top_rules = rules.sort_values('lift', ascending=False).head(top_n)\n",
    "    \n",
    "    # Create network graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add edges (rules)\n",
    "    for _, row in top_rules.iterrows():\n",
    "        antecedents = ', '.join(list(row['antecedents']))\n",
    "        consequents = ', '.join(list(row['consequents']))\n",
    "        \n",
    "        # Add nodes\n",
    "        G.add_node(antecedents)\n",
    "        G.add_node(consequents)\n",
    "        \n",
    "        # Add edge with weight based on lift\n",
    "        G.add_edge(antecedents, consequents, weight=row['lift'], confidence=row['confidence'])\n",
    "    \n",
    "    # Plot network graph - create a new figure\n",
    "    fig_network = plt.figure(figsize=(14, 10))\n",
    "    ax_network = fig_network.add_subplot(111)\n",
    "    \n",
    "    # Define node positions using spring layout\n",
    "    pos = nx.spring_layout(G, k=0.6, seed=42)\n",
    "    \n",
    "    # Get edge weights for width and color\n",
    "    edges = G.edges()\n",
    "    weights = [G[u][v]['weight'] for u, v in edges]\n",
    "    confidence = [G[u][v]['confidence'] for u, v in edges]\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=1500, alpha=0.8, \n",
    "                          node_color='lightblue', linewidths=0.5, edgecolors='black',\n",
    "                          ax=ax_network)\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos, width=[w/5 for w in weights], \n",
    "                          edge_color=confidence, edge_cmap=plt.cm.Blues, \n",
    "                          edge_vmin=min(confidence), edge_vmax=max(confidence), \n",
    "                          arrowsize=20, arrowstyle='->', \n",
    "                          ax=ax_network)\n",
    "    \n",
    "    # Draw labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=9, font_family='sans-serif',\n",
    "                           ax=ax_network)\n",
    "    \n",
    "    ax_network.axis('off')\n",
    "    ax_network.set_title(f'Top {top_n} Association Rules Network', fontsize=16)\n",
    "    \n",
    "    # Add a colorbar for confidence with explicit axis reference\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, \n",
    "                             norm=plt.Normalize(vmin=min(confidence), vmax=max(confidence)))\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax_network, label='Confidence')  # Add ax parameter\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/association_rules_network.png', dpi=300)\n",
    "    print(\"Association rules network saved as 'figure/association_rules_network.png'\")\n",
    "    \n",
    "    # Create heatmap in a new figure\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Extract unique products from top rules\n",
    "    unique_products = set()\n",
    "    for _, row in top_rules.iterrows():\n",
    "        unique_products.update(list(row['antecedents']))\n",
    "        unique_products.update(list(row['consequents']))\n",
    "    \n",
    "    unique_products = list(unique_products)\n",
    "    if len(unique_products) > 15:\n",
    "        unique_products = unique_products[:15]  # Limit to top 15 for readability\n",
    "    \n",
    "    # Create a co-occurrence matrix\n",
    "    cooccurrence = pd.DataFrame(0, index=unique_products, columns=unique_products)\n",
    "    \n",
    "    # Fill the matrix with lift values when rules exist\n",
    "    for _, row in rules.iterrows():\n",
    "        antecedents = list(row['antecedents'])\n",
    "        consequents = list(row['consequents'])\n",
    "        \n",
    "        for a in antecedents:\n",
    "            if a in unique_products:\n",
    "                for c in consequents:\n",
    "                    if c in unique_products:\n",
    "                        cooccurrence.loc[a, c] = max(cooccurrence.loc[a, c], row['lift'])\n",
    "    \n",
    "    # Create heatmap\n",
    "    ax_heatmap = sns.heatmap(cooccurrence, annot=True, cmap='YlGnBu', fmt='.2f')\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.title('Product Association Strength (Lift)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/product_association_heatmap.png', dpi=300)\n",
    "    print(\"Product association heatmap saved as 'figure/product_association_heatmap.png'\")\n",
    "\n",
    "    # Heatmap of top products and their associations\n",
    "    # Extract unique products from top rules\n",
    "    unique_products = set()\n",
    "    for _, row in top_rules.iterrows():\n",
    "        unique_products.update(list(row['antecedents']))\n",
    "        unique_products.update(list(row['consequents']))\n",
    "    \n",
    "    unique_products = list(unique_products)\n",
    "    if len(unique_products) > 15:\n",
    "        unique_products = unique_products[:15]  # Limit to top 15 for readability\n",
    "    \n",
    "    # Create a co-occurrence matrix\n",
    "    cooccurrence = pd.DataFrame(0, index=unique_products, columns=unique_products)\n",
    "    \n",
    "    # Fill the matrix with lift values when rules exist\n",
    "    for _, row in rules.iterrows():\n",
    "        antecedents = list(row['antecedents'])\n",
    "        consequents = list(row['consequents'])\n",
    "        \n",
    "        for a in antecedents:\n",
    "            if a in unique_products:\n",
    "                for c in consequents:\n",
    "                    if c in unique_products:\n",
    "                        cooccurrence.loc[a, c] = max(cooccurrence.loc[a, c], row['lift'])\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    ax = sns.heatmap(cooccurrence, annot=True, cmap='YlGnBu', fmt='.2f')\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.title('Product Association Strength (Lift)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/product_association_heatmap.png', dpi=300)\n",
    "    print(\"Product association heatmap saved as 'product_association_heatmap.png'\")\n",
    "\n",
    "def analyze_basket_size_impact(df, rules):\n",
    "    \"\"\"\n",
    "    Analyze how basket size impacts the strength of associations.\n",
    "    \"\"\"\n",
    "    # Calculate basket size for each transaction\n",
    "    basket_sizes = df.groupby('InvoiceNo')['StockCode'].nunique().reset_index()\n",
    "    basket_sizes.columns = ['InvoiceNo', 'BasketSize']\n",
    "    \n",
    "    # Create basket size categories\n",
    "    basket_sizes['SizeCategory'] = pd.cut(\n",
    "        basket_sizes['BasketSize'],\n",
    "        bins=[0, 2, 5, 10, float('inf')],\n",
    "        labels=['Small (1-2)', 'Medium (3-5)', 'Large (6-10)', 'Very Large (>10)']\n",
    "    )\n",
    "    \n",
    "    # Count transactions by size category\n",
    "    size_distribution = basket_sizes['SizeCategory'].value_counts().sort_index()\n",
    "    \n",
    "    # Plot basket size distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = size_distribution.plot(kind='bar', color='skyblue')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(size_distribution):\n",
    "        ax.text(i, v + 5, str(v), ha='center')\n",
    "    \n",
    "    plt.title('Transaction Count by Basket Size')\n",
    "    plt.xlabel('Basket Size Category')\n",
    "    plt.ylabel('Number of Transactions')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/basket_size_distribution.png', dpi=300)\n",
    "    print(\"Basket size distribution saved as 'basket_size_distribution.png'\")\n",
    "    \n",
    "    # Analyze rule metrics by rule complexity\n",
    "    rules['rule_length'] = rules['antecedent_len'] + rules['consequent_len']\n",
    "    \n",
    "    # Group by rule length and calculate average metrics\n",
    "    rule_complexity = rules.groupby('rule_length').agg({\n",
    "        'support': 'mean',\n",
    "        'confidence': 'mean',\n",
    "        'lift': 'mean',\n",
    "        'leverage': 'mean',\n",
    "        'conviction': 'mean',\n",
    "        'antecedents': 'count'  # Count as number of rules\n",
    "    }).reset_index()\n",
    "    \n",
    "    rule_complexity.rename(columns={'antecedents': 'rule_count'}, inplace=True)\n",
    "    \n",
    "    # Plot metrics by rule complexity\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Support, Confidence, Lift plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    ax1 = plt.gca()\n",
    "    \n",
    "    ax1.plot(rule_complexity['rule_length'], rule_complexity['support'], \n",
    "            marker='o', linestyle='-', color='blue', label='Support')\n",
    "    ax1.plot(rule_complexity['rule_length'], rule_complexity['confidence'], \n",
    "            marker='s', linestyle='-', color='green', label='Confidence')\n",
    "    ax1.set_xlabel('Rule Length (Items)')\n",
    "    ax1.set_ylabel('Average Value')\n",
    "    ax1.set_xticks(rule_complexity['rule_length'])\n",
    "    \n",
    "    # Create a second y-axis for lift (which can have much higher values)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(rule_complexity['rule_length'], rule_complexity['lift'], \n",
    "            marker='^', linestyle='-', color='red', label='Lift')\n",
    "    ax2.set_ylabel('Average Lift')\n",
    "    \n",
    "    # Combine legends\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "    \n",
    "    plt.title('Association Rule Metrics by Rule Length')\n",
    "    \n",
    "    # Rule count by complexity\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(rule_complexity['rule_length'], rule_complexity['rule_count'], \n",
    "           color='purple', alpha=0.7)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(rule_complexity['rule_count']):\n",
    "        plt.text(rule_complexity['rule_length'].iloc[i], v + 0.5, str(int(v)), \n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.xlabel('Rule Length (Items)')\n",
    "    plt.ylabel('Number of Rules')\n",
    "    plt.title('Rule Count by Complexity')\n",
    "    plt.xticks(rule_complexity['rule_length'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/rule_complexity_analysis.png', dpi=300)\n",
    "    print(\"Rule complexity analysis saved as 'rule_complexity_analysis.png'\")\n",
    "\n",
    "def run_market_basket_analysis(file_path, min_support=0.01, min_confidence=0.5):\n",
    "    \"\"\"\n",
    "    Run the complete market basket analysis workflow.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"MARKET BASKET ANALYSIS / ASSOCIATION RULE MINING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    df = load_retail_data(file_path)\n",
    "    \n",
    "    # Create transactions\n",
    "    transaction_lists = create_transactions(df)\n",
    "    \n",
    "    # Create one-hot encoded DataFrame\n",
    "    one_hot_df = create_one_hot_encoded_df(transaction_lists)\n",
    "    \n",
    "    # Find frequent itemsets\n",
    "    frequent_itemsets = find_frequent_itemsets(one_hot_df, min_support)\n",
    "    \n",
    "    # Generate association rules\n",
    "    rules = generate_association_rules(frequent_itemsets, min_confidence)\n",
    "    \n",
    "    # Visualize association rules\n",
    "    visualize_association_rules(rules)\n",
    "    \n",
    "    # Analyze basket size impact\n",
    "    analyze_basket_size_impact(df, rules)\n",
    "    \n",
    "    # Output results to CSV\n",
    "    frequent_itemsets['itemsets_str'] = frequent_itemsets['itemsets'].apply(lambda x: ', '.join(list(x)))\n",
    "    frequent_itemsets.to_csv('data/frequent_itemsets.csv', index=False)\n",
    "    \n",
    "    # Convert frozensets to strings for CSV output\n",
    "    rules_for_export = rules.copy()\n",
    "    rules_for_export['antecedents_str'] = rules_for_export['antecedents'].apply(lambda x: ', '.join(list(x)))\n",
    "    rules_for_export['consequents_str'] = rules_for_export['consequents'].apply(lambda x: ', '.join(list(x)))\n",
    "    \n",
    "    # Export rules\n",
    "    rules_for_export.to_csv('data/association_rules.csv', index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nResults exported to CSV files.\")\n",
    "    \n",
    "    return df, transaction_lists, frequent_itemsets, rules\n",
    "\n",
    "# If running as a script\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the analysis\n",
    "    file_path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx\"  # Update with your file path\n",
    "    \n",
    "    # Set minimum support and confidence thresholds\n",
    "    # You may need to adjust these based on your data\n",
    "    min_support = 0.01    # Items appearing in at least 1% of transactions\n",
    "    min_confidence = 0.5  # Rules that are true at least 50% of the time\n",
    "    \n",
    "    df, transactions, frequent_itemsets, rules = run_market_basket_analysis(\n",
    "        file_path, min_support, min_confidence\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Collaborative Filtering for Product Recommendations (machine-learning-based)\n",
    "\n",
    "Collaborative Filtering (CF) is a machine learning technique used to provide personalized product recommendations based on customer behavior. It works by analyzing past interactions (e.g., purchases, ratings) to suggest items that a user may like.\n",
    "\n",
    "#### How Collaborative Filtering Works\n",
    "1. User-Based Collaborative Filtering\n",
    "    * Finds similar users based on their purchase history.\n",
    "    * Recommends products that similar users have bought but the target user has not.\n",
    "> Example: \"Customers who bought Item A also bought Item B.\"\n",
    "\n",
    "2. Item-Based Collaborative Filtering\n",
    "    * Finds similarities between products based on how often they are purchased together.\n",
    "    * Recommends items that are similar to those a customer has previously bought.\n",
    "> Example: \"If you liked Product X, you might like Product Y.\"\n",
    "\n",
    "3. Matrix Factorization (Advanced CF)\n",
    "    * Uses algorithms like Singular Value Decomposition (SVD) to predict user preferences.\n",
    "    * Commonly used in large-scale recommender systems (e.g., Netflix, Amazon).\n",
    "\n",
    "#### Why Use Collaborative Filtering?\n",
    "* Personalized recommendations improve customer engagement.\n",
    "* Scales well for large datasets.\n",
    "* Learns from real user behavior, making it more dynamic than rule-based methods like Apriori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_theme()\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "def load_and_prepare_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and prepare the retail dataset for recommendation systems.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    print(\"\\nDataset Overview:\")\n",
    "    print(f\"- Shape: {df.shape}\")\n",
    "    print(f\"- Columns: {', '.join(df.columns)}\")\n",
    "    \n",
    "    # Basic data cleaning\n",
    "    # Remove canceled orders (negative quantities)\n",
    "    df = df[df['Quantity'] > 0]\n",
    "    \n",
    "    # Remove missing values\n",
    "    df = df.dropna(subset=['CustomerID', 'StockCode', 'Description'])\n",
    "    \n",
    "    # Convert CustomerID to integer\n",
    "    df['CustomerID'] = df['CustomerID'].astype(int)\n",
    "    \n",
    "    # Filter out non-product items if needed\n",
    "    non_product_codes = ['POST', 'D', 'M', 'CRUK', 'C2']\n",
    "    df = df[~df['StockCode'].isin(non_product_codes)]\n",
    "    \n",
    "    # Filter out items with generic descriptions\n",
    "    generic_descriptions = ['DISCOUNT', 'POSTAGE', 'MANUAL', 'SAMPLES', 'TEST']\n",
    "    df = df[~df['Description'].str.contains('|'.join(generic_descriptions), case=False)]\n",
    "    \n",
    "    print(f\"After cleaning: {df.shape[0]} transactions\")\n",
    "    \n",
    "    # Calculate purchase counts and total item quantity\n",
    "    purchase_counts = df.groupby(['CustomerID', 'StockCode']).size().reset_index(name='Frequency')\n",
    "    purchase_quantities = df.groupby(['CustomerID', 'StockCode'])['Quantity'].sum().reset_index()\n",
    "    \n",
    "    # Merge counts and quantities\n",
    "    customer_item_df = pd.merge(purchase_counts, purchase_quantities, on=['CustomerID', 'StockCode'])\n",
    "    \n",
    "    # Add product descriptions\n",
    "    product_descriptions = df[['StockCode', 'Description']].drop_duplicates()\n",
    "    customer_item_df = pd.merge(customer_item_df, product_descriptions, on='StockCode')\n",
    "    \n",
    "    print(\"\\nCustomer-Item Interaction Dataset:\")\n",
    "    print(f\"- Unique customers: {customer_item_df['CustomerID'].nunique()}\")\n",
    "    print(f\"- Unique products: {customer_item_df['StockCode'].nunique()}\")\n",
    "    print(f\"- Total interactions: {len(customer_item_df)}\")\n",
    "    \n",
    "    # Display distribution of interactions\n",
    "    customer_counts = customer_item_df.groupby('CustomerID').size()\n",
    "    product_counts = customer_item_df.groupby('StockCode').size()\n",
    "    \n",
    "    print(f\"\\nInteractions per customer:\")\n",
    "    print(f\"- Mean: {customer_counts.mean():.2f}\")\n",
    "    print(f\"- Median: {customer_counts.median():.2f}\")\n",
    "    print(f\"- Min: {customer_counts.min()}\")\n",
    "    print(f\"- Max: {customer_counts.max()}\")\n",
    "    \n",
    "    print(f\"\\nCustomers per product:\")\n",
    "    print(f\"- Mean: {product_counts.mean():.2f}\")\n",
    "    print(f\"- Median: {product_counts.median():.2f}\")\n",
    "    print(f\"- Min: {product_counts.min()}\")\n",
    "    print(f\"- Max: {product_counts.max()}\")\n",
    "    \n",
    "    return df, customer_item_df\n",
    "\n",
    "def create_user_item_matrix(customer_item_df, interaction_type='frequency'):\n",
    "    \"\"\"\n",
    "    Create a user-item matrix for collaborative filtering.\n",
    "    \n",
    "    Parameters:\n",
    "    - customer_item_df: DataFrame with customer-item interactions\n",
    "    - interaction_type: 'frequency' or 'quantity' to decide which value to use\n",
    "    \n",
    "    Returns:\n",
    "    - user_item_matrix: Sparse matrix of user-item interactions\n",
    "    - user_mapper: Dictionary mapping user IDs to matrix indices\n",
    "    - item_mapper: Dictionary mapping item IDs to matrix indices\n",
    "    - user_inv_mapper: Dictionary mapping matrix indices to user IDs\n",
    "    - item_inv_mapper: Dictionary mapping matrix indices to item IDs\n",
    "    \"\"\"\n",
    "    if interaction_type == 'frequency':\n",
    "        # Use purchase frequency as interaction value\n",
    "        values = customer_item_df['Frequency']\n",
    "    else:\n",
    "        # Use purchased quantity as interaction value\n",
    "        values = customer_item_df['Quantity']\n",
    "    \n",
    "    # Create mappings of IDs to indices\n",
    "    unique_customers = customer_item_df['CustomerID'].unique()\n",
    "    unique_items = customer_item_df['StockCode'].unique()\n",
    "    \n",
    "    user_mapper = {user_id: i for i, user_id in enumerate(unique_customers)}\n",
    "    item_mapper = {item_id: i for i, item_id in enumerate(unique_items)}\n",
    "    \n",
    "    # Create inverse mappings\n",
    "    user_inv_mapper = {i: user_id for user_id, i in user_mapper.items()}\n",
    "    item_inv_mapper = {i: item_id for item_id, i in item_mapper.items()}\n",
    "    \n",
    "    # Create user and item indices\n",
    "    user_indices = [user_mapper[user_id] for user_id in customer_item_df['CustomerID']]\n",
    "    item_indices = [item_mapper[item_id] for item_id in customer_item_df['StockCode']]\n",
    "    \n",
    "    # Create sparse matrix\n",
    "    user_item_matrix = csr_matrix(\n",
    "        (values, (user_indices, item_indices)),\n",
    "        shape=(len(unique_customers), len(unique_items))\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nUser-Item Matrix Shape: {user_item_matrix.shape}\")\n",
    "    print(f\"Matrix density: {user_item_matrix.nnz / (user_item_matrix.shape[0] * user_item_matrix.shape[1]):.6f}\")\n",
    "    \n",
    "    return user_item_matrix, user_mapper, item_mapper, user_inv_mapper, item_inv_mapper\n",
    "\n",
    "def train_svd_model(user_item_matrix, n_components=50):\n",
    "    \"\"\"\n",
    "    Train a simple SVD model for collaborative filtering.\n",
    "    \"\"\"\n",
    "    # Split data for evaluation\n",
    "    nonzero_indices = user_item_matrix.nonzero()\n",
    "    nonzero_values = user_item_matrix.data\n",
    "    \n",
    "    # Split indices and values into train and test sets\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        range(len(nonzero_values)), test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create train matrix\n",
    "    train_matrix = csr_matrix(user_item_matrix)\n",
    "    for i in test_indices:\n",
    "        row = nonzero_indices[0][i]\n",
    "        col = nonzero_indices[1][i]\n",
    "        train_matrix[row, col] = 0\n",
    "    \n",
    "    # Create test data\n",
    "    test_data = [\n",
    "        (nonzero_indices[0][i], nonzero_indices[1][i], nonzero_values[i])\n",
    "        for i in test_indices\n",
    "    ]\n",
    "    \n",
    "    # Perform SVD on training data\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    user_factors = svd.fit_transform(train_matrix)\n",
    "    item_factors = svd.components_.T\n",
    "    \n",
    "    # Calculate explained variance\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(f\"\\nSVD Model with {n_components} components:\")\n",
    "    print(f\"- Explained variance: {explained_variance:.4f}\")\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    test_predictions = []\n",
    "    for user, item, rating in test_data:\n",
    "        pred = np.dot(user_factors[user], item_factors[item])\n",
    "        test_predictions.append((rating, pred))\n",
    "    \n",
    "    # Calculate RMSE and MAE\n",
    "    rmse = np.sqrt(np.mean([(true - pred) ** 2 for true, pred in test_predictions]))\n",
    "    mae = np.mean([abs(true - pred) for true, pred in test_predictions])\n",
    "    \n",
    "    print(f\"Model Evaluation:\")\n",
    "    print(f\"- RMSE: {rmse:.4f}\")\n",
    "    print(f\"- MAE: {mae:.4f}\")\n",
    "    \n",
    "    return svd, user_factors, item_factors, test_predictions\n",
    "\n",
    "def get_recommendations(user_id, user_item_matrix, user_mapper, item_inv_mapper, \n",
    "                       user_factors, item_factors, df, n=10):\n",
    "    \"\"\"\n",
    "    Get top N product recommendations for a user.\n",
    "    \"\"\"\n",
    "    if user_id not in user_mapper:\n",
    "        print(f\"User {user_id} not found in training data.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Get user index\n",
    "    user_idx = user_mapper[user_id]\n",
    "    \n",
    "    # Get user vector\n",
    "    user_vector = user_factors[user_idx]\n",
    "    \n",
    "    # Calculate predicted ratings for all items\n",
    "    predicted_ratings = np.dot(user_vector, item_factors.T)\n",
    "    \n",
    "    # Get items the user has already interacted with\n",
    "    user_items = set(user_item_matrix[user_idx].nonzero()[1])\n",
    "    \n",
    "    # Create a list of (item_idx, predicted_rating) tuples for items the user hasn't seen\n",
    "    predictions = [\n",
    "        (item_idx, predicted_ratings[item_idx])\n",
    "        for item_idx in range(len(predicted_ratings))\n",
    "        if item_idx not in user_items\n",
    "    ]\n",
    "    \n",
    "    # Sort by predicted rating (descending)\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get top N recommendations\n",
    "    top_n_recs = predictions[:n]\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    rec_df = pd.DataFrame(top_n_recs, columns=['ItemIdx', 'PredictedRating'])\n",
    "    \n",
    "    # Add item ID\n",
    "    rec_df['StockCode'] = rec_df['ItemIdx'].apply(lambda x: item_inv_mapper[x])\n",
    "    \n",
    "    # Add product descriptions\n",
    "    rec_df = pd.merge(\n",
    "        rec_df, \n",
    "        df[['StockCode', 'Description']].drop_duplicates(), \n",
    "        on='StockCode'\n",
    "    )\n",
    "    \n",
    "    return rec_df[['StockCode', 'Description', 'PredictedRating']]\n",
    "\n",
    "def get_similar_items(item_id, item_mapper, item_factors, item_inv_mapper, df, n=10):\n",
    "    \"\"\"\n",
    "    Get top N similar items based on learned item factors.\n",
    "    \"\"\"\n",
    "    if item_id not in item_mapper:\n",
    "        print(f\"Item {item_id} not found in training data.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Get item index\n",
    "    item_idx = item_mapper[item_id]\n",
    "    \n",
    "    # Get item vector\n",
    "    item_vector = item_factors[item_idx]\n",
    "    \n",
    "    # Calculate similarity to all other items\n",
    "    similarities = cosine_similarity([item_vector], item_factors)[0]\n",
    "    \n",
    "    # Create a list of (item_idx, similarity) tuples\n",
    "    item_sims = [\n",
    "        (idx, sim)\n",
    "        for idx, sim in enumerate(similarities)\n",
    "        if idx != item_idx  # Exclude the item itself\n",
    "    ]\n",
    "    \n",
    "    # Sort by similarity (descending)\n",
    "    item_sims.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get top N similar items\n",
    "    top_n_sims = item_sims[:n]\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    sim_df = pd.DataFrame(top_n_sims, columns=['ItemIdx', 'Similarity'])\n",
    "    \n",
    "    # Add item ID\n",
    "    sim_df['StockCode'] = sim_df['ItemIdx'].apply(lambda x: item_inv_mapper[x])\n",
    "    \n",
    "    # Add product descriptions\n",
    "    sim_df = pd.merge(\n",
    "        sim_df, \n",
    "        df[['StockCode', 'Description']].drop_duplicates(), \n",
    "        on='StockCode'\n",
    "    )\n",
    "    \n",
    "    return sim_df[['StockCode', 'Description', 'Similarity']]\n",
    "\n",
    "def visualize_user_item_distribution(customer_item_df):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of user-item interactions.\n",
    "    \"\"\"\n",
    "    # Calculate interaction counts\n",
    "    user_counts = customer_item_df.groupby('CustomerID').size()\n",
    "    item_counts = customer_item_df.groupby('StockCode').size()\n",
    "    \n",
    "    # Create a figure with two subplots\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # Plot user interaction distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(user_counts, kde=True, bins=30)\n",
    "    plt.title('Distribution of Items per Customer', fontsize=14)\n",
    "    plt.xlabel('Number of Items')\n",
    "    plt.ylabel('Number of Customers')\n",
    "    \n",
    "    # Plot item interaction distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(item_counts, kde=True, bins=30)\n",
    "    plt.title('Distribution of Customers per Item', fontsize=14)\n",
    "    plt.xlabel('Number of Customers')\n",
    "    plt.ylabel('Number of Items')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/interaction_distributions.png', dpi=300)\n",
    "    print(\"Interaction distributions saved as 'interaction_distributions.png'\")\n",
    "    \n",
    "    # Create box plots\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # User interactions box plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(y=user_counts)\n",
    "    plt.title('Items per Customer', fontsize=14)\n",
    "    plt.ylabel('Number of Items')\n",
    "    \n",
    "    # Item interactions box plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(y=item_counts)\n",
    "    plt.title('Customers per Item', fontsize=14)\n",
    "    plt.ylabel('Number of Customers')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/interaction_boxplots.png', dpi=300)\n",
    "    print(\"Interaction box plots saved as 'interaction_boxplots.png'\")\n",
    "\n",
    "def visualize_latent_factors(svd, df, item_inv_mapper, n_components=3):\n",
    "    \"\"\"\n",
    "    Visualize the latent factors learned by SVD.\n",
    "    \"\"\"\n",
    "    # Get the first n_components of item factors\n",
    "    item_factors = svd.components_[:n_components].T\n",
    "    \n",
    "    # Get popular items for labeling\n",
    "    item_popularity = np.asarray(df.groupby('StockCode').size())\n",
    "    top_items = df.groupby('StockCode').size().sort_values(ascending=False)[:20].index.tolist()\n",
    "    \n",
    "    # Extract factors for visualization\n",
    "    items_to_plot = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(len(item_inv_mapper)):\n",
    "        item_id = item_inv_mapper[i]\n",
    "        if item_id in top_items:\n",
    "            items_to_plot.append(item_factors[i])\n",
    "            \n",
    "            # Get item description\n",
    "            desc = df[df['StockCode'] == item_id]['Description'].iloc[0]\n",
    "            if len(desc) > 15:\n",
    "                desc = desc[:15] + \"...\"\n",
    "                \n",
    "            labels.append(desc)\n",
    "    \n",
    "    items_to_plot = np.array(items_to_plot)\n",
    "    \n",
    "    # Create 2D or 3D plot based on components\n",
    "    if n_components == 2:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        plt.scatter(items_to_plot[:, 0], items_to_plot[:, 1], alpha=0.7)\n",
    "        \n",
    "        # Add labels\n",
    "        for i, label in enumerate(labels):\n",
    "            plt.annotate(label, (items_to_plot[i, 0], items_to_plot[i, 1]), \n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "        \n",
    "        plt.title('Items in Latent Factor Space (2D)', fontsize=14)\n",
    "        plt.xlabel('Component 1')\n",
    "        plt.ylabel('Component 2')\n",
    "        plt.grid(True)\n",
    "        \n",
    "    elif n_components == 3:\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "        \n",
    "        fig = plt.figure(figsize=(14, 12))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        ax.scatter(items_to_plot[:, 0], items_to_plot[:, 1], items_to_plot[:, 2], alpha=0.7)\n",
    "        \n",
    "        # Add labels\n",
    "        for i, label in enumerate(labels):\n",
    "            ax.text(items_to_plot[i, 0], items_to_plot[i, 1], items_to_plot[i, 2], label, fontsize=9)\n",
    "        \n",
    "        ax.set_title('Items in Latent Factor Space (3D)', fontsize=14)\n",
    "        ax.set_xlabel('Component 1')\n",
    "        ax.set_ylabel('Component 2')\n",
    "        ax.set_zlabel('Component 3')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figure/latent_factors_{n_components}d.png', dpi=300)\n",
    "    print(f\"Latent factors visualization saved as 'latent_factors_{n_components}d.png'\")\n",
    "\n",
    "def visualize_item_similarity(df, item_mapper, item_factors):\n",
    "    \"\"\"\n",
    "    Visualize item similarity based on learned item factors.\n",
    "    \"\"\"\n",
    "    # Get popular items\n",
    "    popular_items = df.groupby('StockCode').size().sort_values(ascending=False)[:10].index.tolist()\n",
    "    \n",
    "    # Calculate similarity matrix for popular items\n",
    "    popular_item_indices = [item_mapper[item] for item in popular_items if item in item_mapper]\n",
    "    \n",
    "    if len(popular_item_indices) < 2:\n",
    "        print(\"Not enough popular items found in the training data.\")\n",
    "        return\n",
    "    \n",
    "    popular_item_factors = item_factors[popular_item_indices]\n",
    "    similarity_matrix = cosine_similarity(popular_item_factors)\n",
    "    \n",
    "    # Get item descriptions\n",
    "    descriptions = {}\n",
    "    for item in popular_items:\n",
    "        if item not in item_mapper:\n",
    "            continue\n",
    "        desc = df[df['StockCode'] == item]['Description'].iloc[0]\n",
    "        if len(desc) > 20:\n",
    "            desc = desc[:20] + \"...\"\n",
    "        descriptions[item] = desc\n",
    "    \n",
    "    # Create item labels\n",
    "    labels = [descriptions[popular_items[i]] for i in range(len(popular_item_indices))]\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(similarity_matrix, annot=True, fmt=\".2f\", cmap='viridis',\n",
    "               xticklabels=labels, yticklabels=labels,\n",
    "               cbar_kws={'label': 'Cosine Similarity'})\n",
    "    plt.title('Item Similarity Between Popular Products', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure/item_similarity_heatmap.png', dpi=300)\n",
    "    print(\"Item similarity heatmap saved as 'item_similarity_heatmap.png'\")\n",
    "\n",
    "def run_collaborative_filtering_analysis(file_path):\n",
    "    \"\"\"\n",
    "    Run the complete collaborative filtering analysis workflow.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COLLABORATIVE FILTERING FOR PRODUCT RECOMMENDATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    df, customer_item_df = load_and_prepare_data(file_path)\n",
    "    \n",
    "    # Create user-item matrix\n",
    "    user_item_matrix, user_mapper, item_mapper, user_inv_mapper, item_inv_mapper = create_user_item_matrix(customer_item_df)\n",
    "    \n",
    "    # Visualize interaction distributions\n",
    "    visualize_user_item_distribution(customer_item_df)\n",
    "    \n",
    "    # Train SVD model\n",
    "    n_components = min(50, min(user_item_matrix.shape[0], user_item_matrix.shape[1]) - 1)\n",
    "    svd, user_factors, item_factors, test_predictions = train_svd_model(user_item_matrix, n_components)\n",
    "    \n",
    "    # Visualize latent factors\n",
    "    if n_components >= 3:\n",
    "        visualize_latent_factors(svd, df, item_inv_mapper, n_components=3)\n",
    "    elif n_components >= 2:\n",
    "        visualize_latent_factors(svd, df, item_inv_mapper, n_components=2)\n",
    "    \n",
    "    # Visualize item similarity\n",
    "    visualize_item_similarity(df, item_mapper, item_factors)\n",
    "    \n",
    "    # Sample users and items for recommendations\n",
    "    sample_users = random.sample(list(user_inv_mapper.values()), min(3, len(user_inv_mapper)))\n",
    "    \n",
    "    print(\"\\nSample User Recommendations:\")\n",
    "    for user_id in sample_users:\n",
    "        print(f\"\\nRecommendations for User {user_id}:\")\n",
    "        \n",
    "        # Get recommendations\n",
    "        recommendations = get_recommendations(\n",
    "            user_id, user_item_matrix, user_mapper, item_inv_mapper, \n",
    "            user_factors, item_factors, df, n=5\n",
    "        )\n",
    "        \n",
    "        if not recommendations.empty:\n",
    "            for i, (_, rec) in enumerate(recommendations.iterrows()):\n",
    "                print(f\"{i+1}. {rec['Description']} (StockCode: {rec['StockCode']}, \"\n",
    "                     f\"Predicted Rating: {rec['PredictedRating']:.3f})\")\n",
    "    \n",
    "    # Sample item similarities\n",
    "    popular_items = df.groupby('StockCode').size().sort_values(ascending=False)[:5].index.tolist()\n",
    "    \n",
    "    print(\"\\nSimilar Item Examples:\")\n",
    "    for item_id in popular_items:\n",
    "        if item_id not in item_mapper:\n",
    "            continue\n",
    "            \n",
    "        item_desc = df[df['StockCode'] == item_id]['Description'].iloc[0]\n",
    "        print(f\"\\nProducts similar to: {item_desc} (StockCode: {item_id})\")\n",
    "        \n",
    "        similar_items = get_similar_items(\n",
    "            item_id, item_mapper, item_factors, item_inv_mapper, df, n=5\n",
    "        )\n",
    "        \n",
    "        if not similar_items.empty:\n",
    "            for i, (_, similar) in enumerate(similar_items.iterrows()):\n",
    "                print(f\"{i+1}. {similar['Description']} (StockCode: {similar['StockCode']}, \"\n",
    "                     f\"Similarity: {similar['Similarity']:.3f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Return key components for further analysis\n",
    "    return df, customer_item_df, svd, user_factors, item_factors, user_mapper, item_mapper, user_inv_mapper, item_inv_mapper\n",
    "\n",
    "# If running as a script\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the analysis\n",
    "    file_path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx\"  # Update with your file path\n",
    "    results = run_collaborative_filtering_analysis(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Wrap-up \n",
    "\n",
    "<blockquote style=\"font-size: 30px; margin-left: 20px;\">\n",
    "\"AI is not magic. It's just good math.\" \n",
    "</blockquote>\n",
    "\n",
    "![AI is not magic](https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/ai_is_not_magic.png)\n",
    "[Image credit: xkcd](https://xkcd.com/1838)\n",
    "\n",
    "## Reference\n",
    "- Ng, M. Y., Kapur, S., Blizinsky, K. D., & Hernandez-Boussard, T. (2022). The AI life cycle: a holistic approach to creating ethical AI for health decisions. Nature medicine, 28(11), 2247-2249. https://doi.org/10.1038/s41591-022-01993-y\n",
    "- Oniani, D., Hilsman, J., Peng, Y., Poropatich, R. K., Pamplin, J. C., Legault, G. L., & Wang, Y. (2023). Adopting and expanding ethical principles for generative artificial intelligence from military to healthcare. NPJ Digital Medicine, 6(1), 225. https://doi.org/10.1038/s41746-023-00965-x\n",
    "- Sarker, I.H. (2024). Generative AI and Large Language Modeling in Cybersecurity. In: AI-Driven Cybersecurity and Threat Intelligence. Springer, Cham. https://doi.org/10.1007/978-3-031-54497-2_5\n",
    "\n",
    "## Future Learning Resources\n",
    "\n",
    "- [Deep Dive into LLMs like ChatGPT](https://youtu.be/7xTGNNLPyMI?si=tuOTm8Zdv8GycV6R) - a general audience deep dive into the Large Language Model (LLM) AI technology that powers ChatGPT and related products. It is covers the full training stack of how the models are developed, along with mental models of how to think about their \"psychology\", and how to get the best use them in practical applications.\n",
    "- [Prompt Engineering](https://www.promptingguide.ai) - a comprehensive resource designed to help users master the art of prompt engineering for AI models. It provides structured guidance, best practices, and practical examples to improve interactions with AI systems like ChatGPT, Claude, and other LLMs.\n",
    "- [Hugging Face](https://huggingface.co/)- a leading platform for open-source AI, specializing in natural language processing (NLP), machine learning models, and AI collaboration tools. It provides a comprehensive ecosystem for developers, researchers, and organizations to build, train, and deploy AI models efficiently.\n",
    "- [Mckinsey Report1 - AI in Company](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/superagency-in-the-workplace-empowering-people-to-unlock-ais-full-potential-at-work) \n",
    "- [Mckinsey Report2 - AI in Company](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Post-measurements\n",
    "\n",
    "## Post-assessments (post knowledge)\n",
    "* [Link](https://ncsu.qualtrics.com/jfe/form/SV_8qfQ1kyxCpcRMrQ)\n",
    "* <img src=\"https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/post_assess.png\" alt=\"QR\" width=\"200\">\n",
    "\n",
    "## Post-survey (post experience)\n",
    "* [Link](https://ncsu.qualtrics.com/jfe/form/SV_6VTIyENQC8rew98)\n",
    "* <img src=\"https://raw.githubusercontent.com/mzhuang3/my-project-images/main/ai-business-analytics-workshops/post_survey.png\" alt=\"QR\" width=\"200\">\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Python_Open_Labs_Week1_filled.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
